"""
RSS æ–°é—»è·å–æ¨¡å—
ä»é…ç½®çš„ RSS æºè·å–æ–°é—»æ–‡ç« 
"""

import time
from dataclasses import dataclass
from datetime import datetime, timedelta
from pathlib import Path
from typing import Optional

import feedparser
import requests
import yaml


@dataclass
class Article:
    """æ–°é—»æ–‡ç« æ•°æ®ç±»"""
    title: str
    summary: str
    link: str
    source: str
    category: str
    published: Optional[datetime] = None

    def __str__(self) -> str:
        return f"[{self.source}] {self.title}"


class RSSFetcher:
    """RSS æ–°é—»è·å–å™¨"""

    def __init__(self, config_path: str = "config/sources.yaml"):
        """
        åˆå§‹åŒ– RSS è·å–å™¨

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.config = self._load_config(config_path)
        self.fetch_config = self.config.get("fetch", {})
        self.filters = self.config.get("filters", {})

    def _load_config(self, config_path: str) -> dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        path = Path(config_path)
        if not path.exists():
            # å°è¯•ä»é¡¹ç›®æ ¹ç›®å½•åŠ è½½
            project_root = Path(__file__).parent.parent.parent
            path = project_root / config_path

        if not path.exists():
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")

        with open(path, "r", encoding="utf-8") as f:
            return yaml.safe_load(f)

    def _get_enabled_sources(self) -> list[dict]:
        """è·å–å¯ç”¨çš„ RSS æº"""
        sources = self.config.get("sources", {}).get("rss", [])
        return [s for s in sources if s.get("enabled", False)]

    def _fetch_feed(self, url: str) -> Optional[feedparser.FeedParserDict]:
        """
        è·å–å•ä¸ª RSS feed

        Args:
            url: RSS feed URL

        Returns:
            è§£æåçš„ feed æˆ– Noneï¼ˆå¤±è´¥æ—¶ï¼‰
        """
        timeout = self.fetch_config.get("timeout", 30)
        user_agent = self.fetch_config.get("user_agent", "DailyPodcastAI/1.0")
        max_retries = self.fetch_config.get("max_retries", 3)

        headers = {"User-Agent": user_agent}

        for attempt in range(max_retries):
            try:
                response = requests.get(url, headers=headers, timeout=timeout)
                response.raise_for_status()
                feed = feedparser.parse(response.content)

                if feed.bozo and not feed.entries:
                    print(f"  âš ï¸ RSS è§£æè­¦å‘Š: {feed.bozo_exception}")
                    continue

                return feed

            except requests.RequestException as e:
                print(f"  âŒ è¯·æ±‚å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    time.sleep(2)

        return None

    def _parse_published_date(self, entry: dict) -> Optional[datetime]:
        """è§£æå‘å¸ƒæ—¶é—´"""
        if hasattr(entry, "published_parsed") and entry.published_parsed:
            try:
                return datetime(*entry.published_parsed[:6])
            except (TypeError, ValueError):
                pass

        if hasattr(entry, "updated_parsed") and entry.updated_parsed:
            try:
                return datetime(*entry.updated_parsed[:6])
            except (TypeError, ValueError):
                pass

        return None

    def _matches_filters(self, article: Article) -> bool:
        """
        æ£€æŸ¥æ–‡ç« æ˜¯å¦ç¬¦åˆè¿‡æ»¤æ¡ä»¶

        Args:
            article: æ–‡ç« å¯¹è±¡

        Returns:
            True å¦‚æœæ–‡ç« åº”è¯¥ä¿ç•™
        """
        include_keywords = self.filters.get("include_keywords", [])
        exclude_keywords = self.filters.get("exclude_keywords", [])

        text = f"{article.title} {article.summary}".lower()

        # æ£€æŸ¥é»‘åå•
        for keyword in exclude_keywords:
            if keyword.lower() in text:
                return False

        # å¦‚æœæœ‰ç™½åå•ï¼Œæ£€æŸ¥æ˜¯å¦åŒ…å«ä»»ä¸€å…³é”®è¯
        if include_keywords:
            for keyword in include_keywords:
                if keyword.lower() in text:
                    return True
            return False

        return True

    def _is_recent(self, article: Article, hours: int = 24) -> bool:
        """æ£€æŸ¥æ–‡ç« æ˜¯å¦åœ¨æŒ‡å®šæ—¶é—´å†…å‘å¸ƒ"""
        if not article.published:
            return True  # æ— å‘å¸ƒæ—¶é—´åˆ™é»˜è®¤ä¿ç•™

        cutoff = datetime.now() - timedelta(hours=hours)
        return article.published >= cutoff

    def fetch_from_source(self, source: dict) -> list[Article]:
        """
        ä»å•ä¸ªæºè·å–æ–‡ç« 

        Args:
            source: æºé…ç½®å­—å…¸

        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        name = source.get("name", "æœªçŸ¥")
        url = source.get("url", "")
        category = source.get("category", "ç»¼åˆ")
        max_articles = self.filters.get("max_articles_per_source", 10)

        print(f"ğŸ“° è·å– {name} ...")

        feed = self._fetch_feed(url)
        if not feed:
            print(f"  âŒ è·å–å¤±è´¥")
            return []

        articles = []
        for entry in feed.entries[:max_articles * 2]:  # è·å–æ›´å¤šä»¥ä¾¿è¿‡æ»¤
            title = entry.get("title", "").strip()
            summary = entry.get("summary", entry.get("description", "")).strip()
            link = entry.get("link", "")

            if not title:
                continue

            # æ¸…ç† HTML æ ‡ç­¾ï¼ˆç®€å•å¤„ç†ï¼‰
            import re
            summary = re.sub(r"<[^>]+>", "", summary)
            summary = summary[:500]  # é™åˆ¶æ‘˜è¦é•¿åº¦

            article = Article(
                title=title,
                summary=summary,
                link=link,
                source=name,
                category=category,
                published=self._parse_published_date(entry)
            )

            # åº”ç”¨è¿‡æ»¤
            if self._matches_filters(article) and self._is_recent(article):
                articles.append(article)

            if len(articles) >= max_articles:
                break

        print(f"  âœ… è·å– {len(articles)} ç¯‡æ–‡ç« ")
        return articles

    def fetch_all(self) -> list[Article]:
        """
        ä»æ‰€æœ‰å¯ç”¨çš„æºè·å–æ–‡ç« 

        Returns:
            æ‰€æœ‰æ–‡ç« åˆ—è¡¨ï¼ˆå·²æ’åºã€å»é‡ã€é™åˆ¶æ•°é‡ï¼‰
        """
        sources = self._get_enabled_sources()
        if not sources:
            print("âš ï¸ æ²¡æœ‰å¯ç”¨çš„ RSS æº")
            return []

        print(f"ğŸš€ å¼€å§‹è·å–æ–°é—»ï¼Œå…± {len(sources)} ä¸ªæº")
        print("-" * 40)

        all_articles = []
        request_interval = self.fetch_config.get("request_interval", 2)

        for i, source in enumerate(sources):
            articles = self.fetch_from_source(source)
            all_articles.extend(articles)

            # è¯·æ±‚é—´éš”
            if i < len(sources) - 1:
                time.sleep(request_interval)

        # å»é‡ï¼ˆåŸºäºæ ‡é¢˜ï¼‰
        seen_titles = set()
        unique_articles = []
        for article in all_articles:
            if article.title not in seen_titles:
                seen_titles.add(article.title)
                unique_articles.append(article)

        # æŒ‰å‘å¸ƒæ—¶é—´æ’åºï¼ˆæœ€æ–°åœ¨å‰ï¼‰
        unique_articles.sort(
            key=lambda a: a.published or datetime.min,
            reverse=True
        )

        # é™åˆ¶æ€»æ•°
        max_total = self.filters.get("max_total_articles", 20)
        final_articles = unique_articles[:max_total]

        print("-" * 40)
        print(f"ğŸ“Š æ€»è®¡: {len(final_articles)} ç¯‡æ–‡ç«  (å»é‡å)")

        return final_articles


def main():
    """æµ‹è¯•å…¥å£"""
    fetcher = RSSFetcher()
    articles = fetcher.fetch_all()

    print("\n" + "=" * 50)
    print("ğŸ“° ä»Šæ—¥æ–°é—»åˆ—è¡¨")
    print("=" * 50)

    for i, article in enumerate(articles, 1):
        print(f"\n{i}. [{article.category}] {article.title}")
        print(f"   æ¥æº: {article.source}")
        if article.published:
            print(f"   æ—¶é—´: {article.published.strftime('%Y-%m-%d %H:%M')}")
        print(f"   æ‘˜è¦: {article.summary[:100]}...")


if __name__ == "__main__":
    main()
