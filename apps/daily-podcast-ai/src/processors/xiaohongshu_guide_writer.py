"""
å°çº¢ä¹¦çƒ­ç‚¹æŒ‡å—ç”Ÿæˆå™¨
åŸºäºæ¯æ—¥ç§‘æŠ€æ–°é—»ï¼Œä¸ºå°çº¢ä¹¦åˆ›ä½œè€…æä¾›è¹­ç‚¹å»ºè®®å’Œæ–‡æ¡ˆæ¨¡æ¿
"""

import json
import os
import re
from collections import Counter
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import List, Optional

from anthropic import Anthropic

# å¯¼å…¥ Article æ•°æ®ç±»
import sys
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root / "src"))

from news_sources.rss_fetcher import Article


@dataclass
class TitleTemplate:
    """æ ‡é¢˜æ¨¡æ¿"""
    text: str                    # æ ‡é¢˜æ–‡æ¡ˆï¼ˆ20-30å­—ï¼‰
    style: str                   # é£æ ¼ï¼š"ç¬¬ä¸€äººç§°"/"æ‚¬å¿µå¼"/"æ•°å­—å¼"
    target_audience: str         # ç›®æ ‡å—ä¼—
    emoji_suggestion: str        # è¡¨æƒ…ç¬¦å·å»ºè®®


@dataclass
class ContentAngle:
    """å†…å®¹è§’åº¦å»ºè®®"""
    angle: str                   # è§’åº¦åç§°ï¼ˆå¦‚"æ•ˆç‡é©å‘½æŒ‡å—"ï¼‰
    why_works: str              # ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼ˆç”¨æˆ·ç—›ç‚¹ï¼‰
    example_outline: str        # å†…å®¹å¤§çº²ç¤ºä¾‹


@dataclass
class XiaohongshuHotspotGuide:
    """å•æ¡çƒ­ç‚¹çš„å®Œæ•´åˆ›ä½œæŒ‡å—"""

    # åŸå§‹æ–°é—»
    original_title: str
    original_summary: str
    source: str
    category: str
    published: Optional[str]
    link: str

    # é€‚é…åº¦è¯„ä¼°
    adaptation_score: int        # 1-5æ˜Ÿ
    adaptation_reason: str       # é€‚é…ç†ç”±
    target_demographics: List[str]  # ç›®æ ‡äººç¾¤

    # åˆ›ä½œèµ„æº
    title_templates: List[TitleTemplate]
    content_angles: List[ContentAngle]
    hashtag_suggestions: List[str]
    image_suggestions: str

    # é£é™©æç¤º
    risk_warning: Optional[str] = None


@dataclass
class XiaohongshuGuideDocument:
    """å®Œæ•´æŒ‡å—æ–‡æ¡£"""
    date: str
    total_hotspots: int

    # æŒ‰é€‚é…åº¦åˆ†ç»„
    high_priority: List[XiaohongshuHotspotGuide] = field(default_factory=list)  # 4-5æ˜Ÿ
    medium_priority: List[XiaohongshuHotspotGuide] = field(default_factory=list)  # 3æ˜Ÿ
    low_priority: List[XiaohongshuHotspotGuide] = field(default_factory=list)  # 1-2æ˜Ÿ

    # è¶‹åŠ¿åˆ†æ
    trend_summary: str = ""
    hot_keywords: List[str] = field(default_factory=list)

    def save_to_markdown(self, output_path: str) -> str:
        """ä¿å­˜ä¸º Markdown æ–‡ä»¶"""
        lines = []

        # === å¤´éƒ¨ ===
        lines.append("# ğŸ“± å°çº¢ä¹¦çƒ­ç‚¹è¹­æ¦œæŒ‡å—")
        lines.append("")
        lines.append(f"**æ—¥æœŸ**ï¼š{self.date}")
        lines.append(f"**çƒ­ç‚¹æ€»æ•°**ï¼š{self.total_hotspots} æ¡")
        if self.hot_keywords:
            lines.append(f"**è¶‹åŠ¿å…³é”®è¯**ï¼š{', '.join(self.hot_keywords[:10])}")
        lines.append("")
        lines.append("---")
        lines.append("")

        # === è¶‹åŠ¿æ´å¯Ÿ ===
        lines.append("## ğŸ”¥ ä»Šæ—¥è¶‹åŠ¿æ´å¯Ÿ")
        lines.append("")
        lines.append(self.trend_summary if self.trend_summary else "ä»Šæ—¥ç§‘æŠ€çƒ­ç‚¹å¤šå…ƒåŒ–ï¼Œæ¶µç›–AIã€æ¶ˆè´¹ã€ç§‘æŠ€ç­‰å¤šä¸ªé¢†åŸŸã€‚")
        lines.append("")
        if self.hot_keywords:
            lines.append(f"**é«˜é¢‘å…³é”®è¯**ï¼š{', '.join(self.hot_keywords)}")
        lines.append("")
        lines.append("---")
        lines.append("")

        # === é«˜é€‚é…åº¦çƒ­ç‚¹ ===
        if self.high_priority:
            lines.append("## â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ é«˜é€‚é…åº¦çƒ­ç‚¹ï¼ˆå¿…åšï¼ï¼‰")
            lines.append("")
            for i, guide in enumerate(self.high_priority, 1):
                lines.extend(self._render_hotspot(guide, i))

        # === ä¸­ç­‰é€‚é…åº¦çƒ­ç‚¹ ===
        if self.medium_priority:
            lines.append("## â­ï¸â­ï¸â­ï¸ ä¸­ç­‰é€‚é…åº¦çƒ­ç‚¹ï¼ˆå¯é€‰ï¼‰")
            lines.append("")
            for i, guide in enumerate(self.medium_priority, 1):
                lines.extend(self._render_hotspot(guide, i))

        # === ä½é€‚é…åº¦çƒ­ç‚¹ ===
        if self.low_priority:
            lines.append("## â­ï¸â­ï¸ ä½é€‚é…åº¦çƒ­ç‚¹ï¼ˆè½¬åŒ–æ€è·¯ï¼‰")
            lines.append("")
            for i, guide in enumerate(self.low_priority, 1):
                lines.extend(self._render_hotspot(guide, i, show_conversion_tips=True))

        # === é™„å½• ===
        lines.append("---")
        lines.append("")
        lines.append("## ğŸ“Š é™„å½•ï¼šåˆ›ä½œä¼˜å…ˆçº§å»ºè®®")
        lines.append("")
        lines.append(f"**æœ¬å‘¨å»ºè®®é€‰é¢˜æ•°é‡**ï¼š2-3æ¡")
        lines.append("")

        if self.high_priority:
            lines.append("**ä¼˜å…ˆçº§æ’åº**ï¼š")
            for i, guide in enumerate(self.high_priority[:3], 1):
                lines.append(f"{i}. {guide.original_title[:40]}...ï¼ˆ{guide.adaptation_score}æ˜Ÿï¼‰")
            lines.append("")

        lines.append("**æ—¶æ•ˆæ€§æç¤º**ï¼š")
        lines.append("- æ¶ˆè´¹ç±»è¯é¢˜å»ºè®®3å¤©å†…å‘å¸ƒï¼ˆçƒ­åº¦è¡°å‡å¿«ï¼‰")
        lines.append("- AIå·¥å…·ç±»å¯é•¿æœŸå‘å¸ƒï¼ˆå¸¸é’å†…å®¹ï¼‰")
        lines.append("")
        lines.append("---")
        lines.append("")
        lines.append(f"**ç”Ÿæˆæ—¶é—´**ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

        # è·å–æ‰€æœ‰æ¥æº
        all_guides = self.high_priority + self.medium_priority + self.low_priority
        sources = list(set(g.source for g in all_guides))
        if sources:
            lines.append(f"**æ•°æ®æ¥æº**ï¼š{', '.join(sources)}")
        lines.append("")

        # å†™å…¥æ–‡ä»¶
        output_file = Path(output_path)
        output_file.parent.mkdir(parents=True, exist_ok=True)

        with open(output_file, "w", encoding="utf-8") as f:
            f.write("\n".join(lines))

        return str(output_file)

    def _render_hotspot(self, guide: XiaohongshuHotspotGuide, index: int,
                        show_conversion_tips: bool = False) -> List[str]:
        """æ¸²æŸ“å•æ¡çƒ­ç‚¹çš„ Markdown"""

        lines = []

        # æ ‡é¢˜
        stars = "â­ï¸" * guide.adaptation_score
        lines.append(f"### {index}. {guide.original_title[:50]}... {stars}")
        lines.append("")

        # å…ƒä¿¡æ¯
        lines.append(f"**åŸæ ‡é¢˜**ï¼š{guide.original_title}")
        lines.append(f"**æ¥æº**ï¼š{guide.source} | **åˆ†ç±»**ï¼š{guide.category}")
        lines.append(f"**é€‚é…ç†ç”±**ï¼š{guide.adaptation_reason}")
        lines.append("")

        # é£é™©æç¤º
        if guide.risk_warning:
            lines.append(f"âš ï¸ **é£é™©æç¤º**ï¼š{guide.risk_warning}")
            lines.append("")

        # ç›®æ ‡äººç¾¤
        if guide.target_demographics:
            lines.append("#### ğŸ¯ ç›®æ ‡äººç¾¤")
            for demo in guide.target_demographics:
                lines.append(f"- {demo}")
            lines.append("")

        # æ ‡é¢˜æ¨¡æ¿
        if guide.title_templates:
            lines.append("#### ğŸ“ æ ‡é¢˜æ¨¡æ¿ï¼ˆå¯ç›´æ¥å¥—ç”¨ï¼‰")
            lines.append("")
            for i, template in enumerate(guide.title_templates, 1):
                lines.append(f"{i}. **{template.text}**")
                lines.append(f"   é£æ ¼ï¼š{template.style} | å—ä¼—ï¼š{template.target_audience} | è¡¨æƒ…ï¼š{template.emoji_suggestion}")
                lines.append("")

        # å†…å®¹è§’åº¦
        if guide.content_angles:
            lines.append("#### ğŸ’¡ å†…å®¹è§’åº¦å»ºè®®")
            lines.append("")
            for angle in guide.content_angles:
                lines.append(f"**{angle.angle}**")
                lines.append(f"âœ… ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼š{angle.why_works}")
                lines.append(f"ğŸ“‹ å†…å®¹å¤§çº²ï¼š")
                for outline_item in angle.example_outline.split("\n"):
                    if outline_item.strip():
                        lines.append(f"   {outline_item.strip()}")
                lines.append("")

        # è¯é¢˜æ ‡ç­¾
        if guide.hashtag_suggestions:
            lines.append("#### ğŸ·ï¸ è¯é¢˜æ ‡ç­¾")
            hashtags = " ".join([f"`#{tag}`" for tag in guide.hashtag_suggestions])
            lines.append(hashtags)
            lines.append("")

        # é…å›¾å»ºè®®
        if guide.image_suggestions:
            lines.append("#### ğŸ“¸ é…å›¾å»ºè®®")
            lines.append(guide.image_suggestions)
            lines.append("")

        # è½¬åŒ–æ€è·¯ï¼ˆä»…ä½é€‚é…åº¦çƒ­ç‚¹æ˜¾ç¤ºï¼‰
        if show_conversion_tips and guide.adaptation_score <= 2:
            lines.append("#### ğŸ”„ è½¬åŒ–æ€è·¯")
            lines.append("")
            lines.append("è¿™æ˜¯ä¸€ä¸ªä½é€‚é…åº¦è¯é¢˜ï¼Œå»ºè®®ä»ä»¥ä¸‹è§’åº¦è½¬åŒ–ï¼š")
            lines.append("- ä»Bç«¯è§†è§’ â†’ Cç«¯è§†è§’ï¼ˆå¦‚ï¼šè¿™ä¸ªæŠ€æœ¯å¦‚ä½•å½±å“æ™®é€šäººç”Ÿæ´»ï¼‰")
            lines.append("- ä»å®è§‚è¶‹åŠ¿ â†’ ä¸ªäººåº”ç”¨ï¼ˆå¦‚ï¼šè¶‹åŠ¿ä¸‹çš„ä¸ªäººæœºä¼šï¼‰")
            lines.append("- ä»ä¸“ä¸šæœ¯è¯­ â†’ ç”Ÿæ´»åœºæ™¯ï¼ˆå¦‚ï¼šç”¨æ¯”å–»è§£é‡ŠæŠ€æœ¯ï¼‰")
            lines.append("")
            lines.append("**è½¬åŒ–éš¾åº¦**ï¼šâ­ï¸â­ï¸â­ï¸â­ï¸ï¼ˆéœ€æ·±åº¦äºŒåˆ›ï¼‰")
            lines.append("**å»ºè®®**ï¼šé™¤éä½ æ˜¯è¯¥å‚ç›´é¢†åŸŸåšä¸»ï¼Œå¦åˆ™ä¸å»ºè®®åš")
            lines.append("")

        lines.append("---")
        lines.append("")

        return lines


class XiaohongshuGuideWriter:
    """å°çº¢ä¹¦çƒ­ç‚¹æŒ‡å—ç”Ÿæˆå™¨"""

    def __init__(self, model_name: str = "claude-sonnet-4-20250514"):
        """åˆå§‹åŒ–ç”Ÿæˆå™¨"""
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            raise ValueError(
                "æœªè®¾ç½® ANTHROPIC_API_KEY ç¯å¢ƒå˜é‡ã€‚\n"
                "è¯·åœ¨ .env æ–‡ä»¶ä¸­é…ç½®: ANTHROPIC_API_KEY=your_key_here"
            )

        self.client = Anthropic(api_key=api_key)
        self.model = model_name

    def generate_guide(
        self,
        articles: List[Article],
        date: Optional[datetime] = None,
        batch_size: int = 10
    ) -> XiaohongshuGuideDocument:
        """
        ç”Ÿæˆå°çº¢ä¹¦çƒ­ç‚¹æŒ‡å—

        Args:
            articles: æ–°é—»æ–‡ç« åˆ—è¡¨
            date: æ—¥æœŸ
            batch_size: æ¯æ‰¹å¤„ç†çš„æ–°é—»æ•°é‡ï¼ˆé¿å…è¾“å‡ºè¿‡é•¿ï¼‰

        Returns:
            å®Œæ•´çš„æŒ‡å—æ–‡æ¡£
        """
        if date is None:
            date = datetime.now()

        date_str = date.strftime("%Y-%m-%d")

        print(f"  ğŸ¤– æ­£åœ¨ä½¿ç”¨ Claude ({self.model}) ç”Ÿæˆå°çº¢ä¹¦çƒ­ç‚¹æŒ‡å—...")
        print(f"  ğŸ“Š è¾“å…¥æ–°é—»æ€»æ•°: {len(articles)}")

        # å¦‚æœæ–°é—»æ•°é‡è¶…è¿‡ batch_sizeï¼Œåˆ†æ‰¹å¤„ç†
        all_guides = []
        total_input_tokens = 0
        total_output_tokens = 0

        num_batches = (len(articles) + batch_size - 1) // batch_size  # å‘ä¸Šå–æ•´

        for batch_idx in range(num_batches):
            start_idx = batch_idx * batch_size
            end_idx = min((batch_idx + 1) * batch_size, len(articles))
            batch_articles = articles[start_idx:end_idx]

            if num_batches > 1:
                print(f"  ğŸ“¦ å¤„ç†æ‰¹æ¬¡ {batch_idx + 1}/{num_batches}ï¼ˆç¬¬ {start_idx + 1}-{end_idx} æ¡ï¼‰")

            # 1. æ„å»º Prompt
            system_prompt = self._build_system_prompt()
            user_prompt = self._build_user_prompt(batch_articles, date_str)

            # 2. è°ƒç”¨ Claude API
            try:
                response = self.client.messages.create(
                    model=self.model,
                    max_tokens=16384,
                    messages=[
                        {"role": "user", "content": user_prompt}
                    ],
                    system=system_prompt,
                    temperature=0.7
                )

                # ç´¯è®¡ token æ¶ˆè€—
                total_input_tokens += response.usage.input_tokens
                total_output_tokens += response.usage.output_tokens

                # 3. è§£æ JSON
                raw_text = response.content[0].text.strip()
                guides_data = self._parse_json_response(raw_text)

                # 4. è½¬æ¢ä¸ºæ•°æ®ç±»
                batch_guides = self._convert_to_dataclasses(guides_data)
                all_guides.extend(batch_guides)

            except Exception as e:
                print(f"  âš ï¸ æ‰¹æ¬¡ {batch_idx + 1} å¤±è´¥: {e}")
                # ç»§ç»­å¤„ç†ä¸‹ä¸€æ‰¹
                continue

        # è®°å½•æ€» token æ¶ˆè€—å’Œæˆæœ¬
        print(f"  ğŸ’° æ€» Token æ¶ˆè€—: è¾“å…¥ {total_input_tokens}, è¾“å‡º {total_output_tokens}")
        input_cost = total_input_tokens / 1_000_000 * 3  # $3/M
        output_cost = total_output_tokens / 1_000_000 * 15  # $15/M
        total_cost = input_cost + output_cost
        print(f"  ğŸ’µ æ€»æˆæœ¬: ${total_cost:.4f} (Â¥{total_cost * 7:.2f})")

        # 5. ç”Ÿæˆè¶‹åŠ¿æ´å¯Ÿ
        trend_summary, hot_keywords = self._generate_trend_insights(articles)

        # 6. æŒ‰é€‚é…åº¦åˆ†ç»„
        high = [g for g in all_guides if g.adaptation_score >= 4]
        medium = [g for g in all_guides if g.adaptation_score == 3]
        low = [g for g in all_guides if g.adaptation_score <= 2]

        print(f"  âœ… ç”Ÿæˆå®Œæˆ: {len(high)}æ¡é«˜é€‚é… | {len(medium)}æ¡ä¸­ç­‰ | {len(low)}æ¡ä½é€‚é…")

        return XiaohongshuGuideDocument(
            date=date_str,
            total_hotspots=len(all_guides),
            high_priority=high,
            medium_priority=medium,
            low_priority=low,
            trend_summary=trend_summary,
            hot_keywords=hot_keywords
        )

    def _build_system_prompt(self) -> str:
        """æ„å»º System Prompt"""
        return """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„å°çº¢ä¹¦å†…å®¹ç­–ç•¥ä¸“å®¶ï¼Œæ‹¥æœ‰3å¹´ä»¥ä¸Šå°çº¢ä¹¦åˆ›ä½œå’Œè¿è¥ç»éªŒã€‚

## ä½ çš„ä¸“ä¸šé¢†åŸŸ
1. **å¹³å°æ´å¯Ÿ**ï¼šæ·±åˆ»ç†è§£å°çº¢ä¹¦ç”¨æˆ·ç”»åƒ
   - æ ¸å¿ƒç”¨æˆ·ï¼š18-35å²ï¼Œ70%å¥³æ€§
   - å…´è¶£é¢†åŸŸï¼šç¾å¦†ã€æ—¶å°šã€ç”Ÿæ´»ã€èŒåœºã€ç§‘æŠ€ã€æ¯å©´
   - å†…å®¹åå¥½ï¼šå®ç”¨æ”»ç•¥ã€é¿å‘æŒ‡å—ã€æƒ…ç»ªå…±é¸£ã€é¢œå€¼è‡³ä¸Š

2. **çˆ†æ¬¾è§„å¾‹**ï¼šç†Ÿæ‚‰å°çº¢ä¹¦ç®—æ³•æœºåˆ¶
   - æ ‡é¢˜è¦æœ‰"é’©å­"ï¼ˆæ‚¬å¿µ/åˆ©ç›Š/æƒ…ç»ªï¼‰
   - å†…å®¹è¦æœ‰ä»·å€¼ï¼ˆå¹²è´§/æ–°çŸ¥/å…±é¸£ï¼‰
   - å°é¢è¦æœ‰å†²å‡»åŠ›ï¼ˆå¯¹æ¯”/æ•°å­—/äººè„¸ï¼‰

3. **æƒ…ç»ªå…±é¸£**ï¼šå–„äºå°†ç¡¬æ ¸ç§‘æŠ€æ–°é—»è½¬åŒ–ä¸ºç”¨æˆ·å…³å¿ƒçš„è¯é¢˜
   - ç§‘æŠ€æ–°é—» â†’ "è¿™å¯¹æˆ‘æœ‰ä»€ä¹ˆç”¨ï¼Ÿ"
   - Bç«¯äº§å“ â†’ "æ™®é€šäººèƒ½ç”¨ä¸Šå—ï¼Ÿ"
   - èèµ„æ¶ˆæ¯ â†’ "èƒŒåçš„èµšé’±æœºä¼šåœ¨å“ªï¼Ÿ"

## å°çº¢ä¹¦æ ‡é¢˜å†™ä½œå‡†åˆ™ï¼ˆå¿…é¡»ä¸¥æ ¼éµå¾ªï¼‰

### é•¿åº¦æ§åˆ¶
- æœ€ä½³ï¼š20-30å­—
- ç¦æ­¢ï¼šè¶…è¿‡30å­—æˆ–å°‘äº20å­—

### å¿…å¤‡å…ƒç´ ï¼ˆè‡³å°‘åŒ…å«2ä¸ªï¼‰
1. **ç¬¬ä¸€äººç§°**ï¼š"æˆ‘å®æµ‹""æˆ‘å‘ç°""æˆ‘è¸©å‘""äº²æµ‹æœ‰æ•ˆ"
2. **æ‚¬å¿µåˆ¶é€ **ï¼š"å±…ç„¶...""æ²¡æƒ³åˆ°...""éœ‡æƒŠ...""çœŸç›¸æ˜¯..."
3. **åˆ©ç›Šæ‰¿è¯º**ï¼š"çœäº†XXX""èµšäº†XXX""æ•ˆç‡æå‡XXX"
4. **æƒ…ç»ªè¯æ±‡**ï¼š"emoäº†""ç»äº†""çˆ±äº†""ä¸Šå¤´""ç‹ ç‹ å¿ƒåŠ¨"
5. **æ•°å­—åŒ–è¡¨è¾¾**ï¼š"3ä¸ªæŠ€å·§""7å¤©è§æ•ˆ""æœˆå…¥5w"

### è¡¨æƒ…ç¬¦å·ä½¿ç”¨è§„èŒƒ
- æ¯æ¡æ ‡é¢˜ï¼š1-3ä¸ªè¡¨æƒ…
- å¸¸ç”¨ï¼šâœ¨ğŸ’¡ğŸ”¥ğŸ’°ğŸ˜­â¤ï¸ğŸ“ŠğŸ¯âš¡ï¸
- ç¦æ­¢ï¼šè¿‡åº¦å †ç Œï¼ˆè¶…è¿‡3ä¸ªï¼‰

### å…¸å‹å¥å¼æ¨¡æ¿
1. **æ‚¬å¿µ+åˆ©ç›Š**ï¼š"XXåæˆ‘å‘ç°äº†XXï¼ŒXXXç›´æ¥ç¿»å€âœ¨"
2. **å¯¹æ¯”+æƒ…ç»ª**ï¼š"XX vs XXï¼Œçœ‹å®Œæˆ‘emoäº†ğŸ˜­"
3. **æ•°å­—+æ‰¿è¯º**ï¼š"Xå¤©XXXï¼Œäº²æµ‹æœ‰æ•ˆï¼é™„æ•™ç¨‹ğŸ“Š"
4. **ç¬¬ä¸€äººç§°+æƒŠè®¶**ï¼š"æˆ‘è¯•äº†è¿™ä¸ªXXï¼Œæ²¡æƒ³åˆ°ç«Ÿç„¶XXğŸ”¥"

## é€‚é…åº¦è¯„åˆ†æ ‡å‡†ï¼ˆ1-5æ˜Ÿï¼‰

**â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸ï¼ˆ5æ˜Ÿ - å¿…åšï¼‰**
- ä¸å°çº¢ä¹¦æ ¸å¿ƒäººç¾¤å¼ºç›¸å…³ï¼ˆèŒåœº/æ¶ˆè´¹/ç¾å¦†/ç”Ÿæ´»ç§‘æŠ€ï¼‰
- æœ‰æ˜ç¡®ç”¨æˆ·ç—›ç‚¹æˆ–æƒ…ç»ªå…±é¸£ç‚¹
- å¯ç›´æ¥è½¬åŒ–ä¸ºå®æ“å†…å®¹ï¼ˆæ•™ç¨‹/æµ‹è¯„/æ”»ç•¥ï¼‰
- ç¤ºä¾‹ï¼šAIåˆ›ä½œå·¥å…·ã€æ¶ˆè´¹å“æµ‹è¯„ã€èŒåœºå·¥å…·

**â­ï¸â­ï¸â­ï¸â­ï¸ï¼ˆ4æ˜Ÿ - æ¨èï¼‰**
- è¯é¢˜æœ‰ä¸€å®šçƒ­åº¦ï¼Œä¸ç”¨æˆ·ç”Ÿæ´»ç›¸å…³
- å¯è½¬åŒ–ä¸ºé¿å‘æŒ‡å—æˆ–çœé’±æ”»ç•¥
- ç¤ºä¾‹ï¼šå¤–å–å¤§æˆ˜ã€æ–°å“å‘å¸ƒã€è¡Œä¸šå˜åŒ–

**â­ï¸â­ï¸â­ï¸ï¼ˆ3æ˜Ÿ - å¯åšï¼‰**
- éœ€è¦ä¸€å®šè§’åº¦è½¬åŒ–ï¼Œä½†æœ‰æ½œåœ¨ä»·å€¼
- å¯èƒ½åªé€‚åˆç‰¹å®šå‚ç›´é¢†åŸŸ
- ç¤ºä¾‹ï¼šç§‘æŠ€å…¬å¸èèµ„ã€è¡Œä¸šæŠ¥å‘Š

**â­ï¸â­ï¸ï¼ˆ2æ˜Ÿ - æ…åšï¼‰**
- ä¸ç›®æ ‡äººç¾¤å…³è”è¾ƒå¼±
- éœ€è¦å¤§é‡åŠ å·¥å’Œè§’åº¦è½¬åŒ–
- ç¤ºä¾‹ï¼šBç«¯SaaSã€ä¾›åº”é“¾ç®¡ç†

**â­ï¸ï¼ˆ1æ˜Ÿ - ä¸å»ºè®®ï¼‰**
- çº¯Bç«¯/æ”¿ç­–/è´¢ç»è¯é¢˜
- éš¾ä»¥é€‚é…å°çº¢ä¹¦åœºæ™¯
- ç¤ºä¾‹ï¼šé‡‘èç›‘ç®¡æ”¿ç­–ã€å®è§‚ç»æµæ•°æ®

## è¾“å‡ºæ ¼å¼è¦æ±‚

**é‡è¦**ï¼šå¿…é¡»è¾“å‡ºåˆæ³•çš„ JSON æ•°ç»„ã€‚JSON å­—ç¬¦ä¸²ä¸­çš„å¼•å·ã€åæ–œæ ç­‰ç‰¹æ®Šå­—ç¬¦å¿…é¡»è½¬ä¹‰ï¼š
- åŒå¼•å· " â†’ \\"
- åæ–œæ  \\ â†’ \\\\
- æ¢è¡Œç¬¦ â†’ \\n

æ¯ä¸ªå…ƒç´ åŒ…å«ä»¥ä¸‹å­—æ®µï¼š

```json
[
  {
    "original_title": "æ–°é—»åŸæ ‡é¢˜",
    "original_summary": "æ–°é—»æ‘˜è¦ï¼ˆå‰300å­—ï¼‰",
    "source": "36æ°ª",
    "category": "ç§‘æŠ€",
    "published": "2026-01-15T10:42:30",
    "link": "https://...",

    "adaptation_score": 5,
    "adaptation_reason": "AIåˆ›ä½œå·¥å…·ä¸å°çº¢ä¹¦ç”¨æˆ·ï¼ˆå†…å®¹åˆ›ä½œè€…ï¼‰å¼ºç›¸å…³ï¼Œå®¹æ˜“äº§ç”Ÿ'æˆ‘ä¹Ÿè¦è¯•è¯•'çš„è½¬åŒ–",
    "target_demographics": ["å†…å®¹åˆ›ä½œè€…", "è®¾è®¡çˆ±å¥½è€…", "å‰¯ä¸šæ¢ç´¢è€…"],

    "title_templates": [
      {
        "text": "å®æµ‹ï¼è¿™ä¸ªAIå·¥å…·è®©æˆ‘10åˆ†é’Ÿåšå‡ºçˆ†æ¬¾å°é¢âœ¨",
        "style": "ç¬¬ä¸€äººç§°å®æµ‹",
        "target_audience": "å†…å®¹åˆ›ä½œè€…",
        "emoji_suggestion": "âœ¨ğŸ’¡ğŸ”¥"
      }
    ],

    "content_angles": [
      {
        "angle": "æ•ˆç‡é©å‘½æŒ‡å—",
        "why_works": "èŒåœºäººç—›ç‚¹ï¼šåšå›¾è€—æ—¶ã€ç¼ºä¹è®¾è®¡æŠ€èƒ½",
        "example_outline": "1. ä¼ ç»Ÿåšå›¾vs AIåšå›¾æ—¶é—´å¯¹æ¯”\\n2. 3ä¸ªå®æµ‹æ¡ˆä¾‹å±•ç¤º\\n3. ä¿å§†çº§ä½¿ç”¨æ•™ç¨‹\\n4. è¸©å‘é¿é›·ç»éªŒ"
      }
    ],

    "hashtag_suggestions": ["AIå·¥å…·", "è®¾è®¡ç¥å™¨", "æ•ˆç‡æå‡", "å‰¯ä¸šèµšé’±"],
    "image_suggestions": "å¯¹æ¯”å›¾ï¼šä¼ ç»Ÿè®¾è®¡è½¯ä»¶ç•Œé¢ vs AIå·¥å…·ç”Ÿæˆæ•ˆæœï¼Œçªå‡º'ç®€å•''å¿«é€Ÿ'",
    "risk_warning": null
  }
]
```

## å…³é”®åŸåˆ™
- ä¿ç•™æ‰€æœ‰çƒ­ç‚¹ï¼ˆä¸ç­›é€‰ï¼‰ï¼Œä½†è¯šå®æ ‡æ³¨é€‚é…åº¦
- ä¸ºä½é€‚é…åº¦çƒ­ç‚¹æä¾›"è½¬åŒ–æ€è·¯"
- æ ‡é¢˜å¿…é¡»å¯æ“ä½œï¼ˆç”¨æˆ·çœ‹äº†èƒ½ç«‹åˆ»åŠ¨æ‰‹ï¼‰
- é¿å…ç”Ÿç¡¬çš„å“ç‰Œæ¤å…¥
- è¯†åˆ«æ•æ„Ÿè¯é¢˜ï¼ˆæ”¿æ²»/é‡‘èç›‘ç®¡/è´Ÿé¢ï¼‰ï¼Œåœ¨ risk_warning æ ‡æ³¨
"""

    def _build_user_prompt(self, articles: List[Article], date_str: str) -> str:
        """æ„å»º User Prompt"""

        # å‡†å¤‡æ–°é—»ä¸Šä¸‹æ–‡ï¼ˆç²¾ç®€ç‰ˆï¼Œé¿å…è¶…tokené™åˆ¶å’Œç‰¹æ®Šå­—ç¬¦ï¼‰
        news_context = []
        for i, article in enumerate(articles, 1):
            # æ‘˜è¦æˆªæ–­åˆ°150å­—ï¼ˆå‡å°‘ç‰¹æ®Šå­—ç¬¦é—®é¢˜ï¼‰
            summary = article.summary[:150]
            if len(article.summary) > 150:
                summary += "..."

            # æ¸…ç†å¯èƒ½å¯¼è‡´ JSON è§£æé—®é¢˜çš„å­—ç¬¦
            summary = summary.replace('"', "'").replace('"', "'").replace('"', "'")  # æ›¿æ¢ä¸­æ–‡å¼•å·ä¸ºè‹±æ–‡å•å¼•å·
            summary = summary.replace('\n', ' ').replace('\r', ' ')  # ç§»é™¤æ¢è¡Œç¬¦

            # æ ¼å¼åŒ–å‘å¸ƒæ—¶é—´
            published_str = ""
            if article.published:
                if isinstance(article.published, datetime):
                    published_str = article.published.isoformat()
                else:
                    published_str = str(article.published)

            news_context.append(f"""### æ–°é—» {i}: {article.title}
- **æ¥æº**: {article.source} | **åˆ†ç±»**: {article.category}
- **å‘å¸ƒæ—¶é—´**: {published_str}
- **æ‘˜è¦**: {summary}
- **é“¾æ¥**: {article.link}
""")

        user_prompt = f"""ä»Šå¤©æ˜¯ {date_str}ã€‚

è¯·å°†ä»¥ä¸‹ {len(articles)} æ¡ç§‘æŠ€/å•†ä¸šæ–°é—»è½¬åŒ–ä¸ºå°çº¢ä¹¦åˆ›ä½œè€…çš„çƒ­ç‚¹é€‰é¢˜æŒ‡å—ã€‚

{''.join(news_context)}

---

## ä½ çš„ä»»åŠ¡

**ä¸ºæ¯æ¡æ–°é—»ç”Ÿæˆå®Œæ•´çš„åˆ›ä½œæŒ‡å—**ï¼ŒåŒ…å«ï¼š

1. **é€‚é…åº¦è¯„ä¼°**ï¼ˆ1-5æ˜Ÿï¼‰
   - è¯„ä¼°è¯¥çƒ­ç‚¹ä¸å°çº¢ä¹¦ç”¨æˆ·çš„ç›¸å…³åº¦
   - ç»™å‡ºè¯„åˆ†ç†ç”±ï¼ˆ50-100å­—ï¼‰
   - æ ‡æ³¨ç›®æ ‡äººç¾¤

2. **æ ‡é¢˜æ¨¡æ¿**ï¼ˆæ°å¥½3ä¸ªï¼‰
   - æ¯ä¸ªæ ‡é¢˜20-30å­—
   - å¿…é¡»ç¬¦åˆå°çº¢ä¹¦é£æ ¼ï¼ˆç¬¬ä¸€äººç§°ã€æ‚¬å¿µã€è¡¨æƒ…ç¬¦å·ï¼‰
   - æ ‡æ³¨é£æ ¼ç±»å‹å’Œç›®æ ‡å—ä¼—

3. **å†…å®¹è§’åº¦**ï¼ˆæ°å¥½2ä¸ªï¼‰
   - æä¾›å…·ä½“çš„åˆ›ä½œè§’åº¦ï¼ˆå¦‚"è–…ç¾Šæ¯›æŒ‡å—""é¿å‘æµ‹è¯„"ï¼‰
   - åˆ†æä¸ºä»€ä¹ˆè¿™ä¸ªè§’åº¦æœ‰æ•ˆï¼ˆç”¨æˆ·ç—›ç‚¹/æƒ…ç»ªå…±é¸£ç‚¹ï¼‰
   - ç»™å‡ºå†…å®¹å¤§çº²ç¤ºä¾‹ï¼ˆç®€æ´çš„3-4ä¸ªè¦ç‚¹ï¼‰

4. **è¯é¢˜æ ‡ç­¾**ï¼ˆæ°å¥½5ä¸ªï¼‰
   - æ¨èç›¸å…³çš„å°çº¢ä¹¦è¯é¢˜æ ‡ç­¾
   - ä¼˜å…ˆé€‰æ‹©çƒ­é—¨æ ‡ç­¾

5. **é…å›¾å»ºè®®**
   - æè¿°é€‚åˆçš„é…å›¾é£æ ¼æˆ–å…ƒç´ 

6. **é£é™©æç¤º**ï¼ˆå¦‚æœ‰ï¼‰
   - è¯†åˆ«æ•æ„Ÿè¯é¢˜ï¼ˆæ”¿æ²»ã€é‡‘èç›‘ç®¡ã€è´Ÿé¢äº‹ä»¶ï¼‰
   - ç»™å‡ºè§„é¿å»ºè®®

## ç‰¹åˆ«å¼ºè°ƒ

- âœ… **ä¿ç•™æ‰€æœ‰ {len(articles)} æ¡æ–°é—»**ï¼ˆå³ä½¿é€‚é…åº¦åªæœ‰1æ˜Ÿï¼‰
- âœ… **ä¸ºä½é€‚é…åº¦çƒ­ç‚¹æä¾›è½¬åŒ–æ€è·¯**ï¼ˆå¦‚ä½•ä»Bç«¯è½¬åŒ–ä¸ºCç«¯è§†è§’ï¼‰
- âœ… **æ ‡é¢˜å¿…é¡»å¯ç›´æ¥ä½¿ç”¨**ï¼ˆåˆ›ä½œè€…å¤åˆ¶ç²˜è´´å³å¯å‘å¸ƒï¼‰
- âŒ **é¿å…ç”Ÿç¡¬çš„å“ç‰Œæ¤å…¥**ï¼ˆä¸è¦åƒå¹¿å‘Šæ–‡æ¡ˆï¼‰

**è¾“å‡ºè¦æ±‚**ï¼š
- ç›´æ¥è¾“å‡º JSON æ•°ç»„ï¼Œä¸è¦ Markdown ä»£ç å—æ ‡è®°
- JSON ä¸­çš„ç‰¹æ®Šå­—ç¬¦å¿…é¡»è½¬ä¹‰ï¼šåŒå¼•å· " â†’ \\", åæ–œæ  \\ â†’ \\\\
- ç¡®ä¿è¾“å‡ºæ˜¯åˆæ³•çš„ JSON æ ¼å¼
"""

        return user_prompt

    def _parse_json_response(self, text: str) -> list:
        """
        è§£æ JSON å“åº”ï¼Œå¤„ç†å¯èƒ½çš„æ ¼å¼é—®é¢˜

        å¤ç”¨è‡ª claude_dialogue_writer.py:255-274ï¼Œå¢å¼ºå®¹é”™èƒ½åŠ›

        å®¹é”™ç­–ç•¥ï¼š
        1. å°è¯•ç›´æ¥è§£æ
        2. æ­£åˆ™æå– [...] å—
        3. ç§»é™¤ Markdown æ ‡è®°åè§£æ
        4. ä½¿ç”¨ json5 å®½æ¾è§£æï¼ˆå¦‚æœå¯ç”¨ï¼‰
        """
        # å°è¯•ç›´æ¥è§£æ
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        # å°è¯•æå– JSON å—
        json_match = re.search(r'\[[\s\S]*\]', text)
        if json_match:
            try:
                return json.loads(json_match.group())
            except json.JSONDecodeError:
                pass

        # ç§»é™¤ markdown ä»£ç å—æ ‡è®°
        cleaned = text.replace("```json", "").replace("```", "").strip()
        try:
            return json.loads(cleaned)
        except json.JSONDecodeError:
            pass

        # å°è¯•ä½¿ç”¨ json5ï¼ˆæ›´å®½æ¾çš„JSONè§£æï¼‰
        try:
            import json5
            return json5.loads(text)
        except (ImportError, Exception):
            pass

        # æœ€åçš„å°è¯•ï¼šä½¿ç”¨ demjson3
        try:
            import demjson3
            return demjson3.decode(text)
        except (ImportError, Exception) as e:
            # ä¿å­˜åŸå§‹å“åº”åˆ°æ—¥å¿—
            error_log = f"logs/xiaohongshu_parse_error_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            Path("logs").mkdir(exist_ok=True)
            with open(error_log, "w", encoding="utf-8") as f:
                f.write(f"=== è§£æå¤±è´¥ ===\n")
                f.write(f"æ—¶é—´: {datetime.now()}\n")
                f.write(f"é”™è¯¯: {e}\n\n")
                f.write(f"=== åŸå§‹å“åº” ===\n")
                f.write(text)

            print(f"  âš ï¸ JSON è§£æå¤±è´¥ï¼ŒåŸå§‹å“åº”å·²ä¿å­˜åˆ°: {error_log}")
            print(f"  ğŸ’¡ æç¤ºï¼šå¯å°è¯•å®‰è£… demjson3 åº“æ¥æå‡è§£æå®¹é”™æ€§ï¼špip install demjson3")
            raise

    def _convert_to_dataclasses(self, guides_data: list) -> List[XiaohongshuHotspotGuide]:
        """å°† JSON è½¬æ¢ä¸ºæ•°æ®ç±»"""
        guides = []

        for item in guides_data:
            try:
                # è½¬æ¢åµŒå¥—çš„ TitleTemplate
                title_templates = []
                for t in item.get("title_templates", []):
                    title_templates.append(TitleTemplate(
                        text=t["text"],
                        style=t.get("style", "æœªçŸ¥"),
                        target_audience=t.get("target_audience", "é€šç”¨"),
                        emoji_suggestion=t.get("emoji_suggestion", "")
                    ))

                # è½¬æ¢åµŒå¥—çš„ ContentAngle
                content_angles = []
                for a in item.get("content_angles", []):
                    content_angles.append(ContentAngle(
                        angle=a["angle"],
                        why_works=a.get("why_works", ""),
                        example_outline=a.get("example_outline", "")
                    ))

                # æ„å»ºä¸»å¯¹è±¡
                guide = XiaohongshuHotspotGuide(
                    original_title=item["original_title"],
                    original_summary=item.get("original_summary", ""),
                    source=item.get("source", ""),
                    category=item.get("category", ""),
                    published=item.get("published"),
                    link=item.get("link", ""),
                    adaptation_score=item["adaptation_score"],
                    adaptation_reason=item["adaptation_reason"],
                    target_demographics=item.get("target_demographics", []),
                    title_templates=title_templates,
                    content_angles=content_angles,
                    hashtag_suggestions=item.get("hashtag_suggestions", []),
                    image_suggestions=item.get("image_suggestions", ""),
                    risk_warning=item.get("risk_warning")
                )
                guides.append(guide)

            except KeyError as e:
                print(f"  âš ï¸ è·³è¿‡æ ¼å¼é”™è¯¯çš„çƒ­ç‚¹: {e}")
                continue

        return guides

    def _generate_trend_insights(self, articles: List[Article]) -> tuple[str, List[str]]:
        """
        ç”Ÿæˆè¶‹åŠ¿æ´å¯Ÿ

        Returns:
            (è¶‹åŠ¿æ€»ç»“, é«˜é¢‘å…³é”®è¯åˆ—è¡¨)
        """
        # é¢„å®šä¹‰å…³é”®è¯åº“ï¼ˆç§‘æŠ€/å•†ä¸šé¢†åŸŸï¼‰
        tech_keywords = [
            # AI & ç§‘æŠ€
            "AI", "äººå·¥æ™ºèƒ½", "å¤§æ¨¡å‹", "GPT", "Claude", "æ™ºèƒ½ä½“",
            "èŠ¯ç‰‡", "åŠå¯¼ä½“", "RISC-V",
            "æœºå™¨äºº", "è‡ªåŠ¨é©¾é©¶", "æ–°èƒ½æº",

            # å•†ä¸š & æ¶ˆè´¹
            "èèµ„", "IPO", "ä¸Šå¸‚", "æŠ•èµ„", "ä¼°å€¼",
            "å¤–å–", "æ¶ˆè´¹", "ä¾›åº”é“¾", "ç”µå•†",
            "å“ç‰Œ", "å‡ºæµ·", "è¥æ”¶",

            # è¡Œä¸š
            "æ±½è½¦", "é‡‘è", "åŒ»ç–—", "æ•™è‚²", "æˆ¿åœ°äº§"
        ]

        keyword_counts = Counter()

        # ç»Ÿè®¡å…³é”®è¯å‡ºç°æ¬¡æ•°
        for article in articles:
            text = f"{article.title} {article.summary}".lower()
            for kw in tech_keywords:
                if kw.lower() in text:
                    keyword_counts[kw] += 1

        # æå– Top 10
        hot_keywords = [kw for kw, _ in keyword_counts.most_common(10)]

        # ç®€å•çš„è¶‹åŠ¿æ€»ç»“
        if hot_keywords:
            top3 = ', '.join(hot_keywords[:3])
            trend_summary = (
                f"ä»Šæ—¥ç§‘æŠ€çƒ­ç‚¹èšç„¦äº {top3} ç­‰é¢†åŸŸï¼Œ"
                f"å…±æ”¶å½• {len(articles)} æ¡æ–°é—»ã€‚"
                f"å…¶ä¸­ AI ç›¸å…³è¯é¢˜å æ¯”è¾ƒé«˜ï¼Œé€‚åˆå°çº¢ä¹¦ç§‘æŠ€å†…å®¹åˆ›ä½œè€…é‡ç‚¹å…³æ³¨ã€‚"
            )
        else:
            trend_summary = f"ä»Šæ—¥ç§‘æŠ€çƒ­ç‚¹å¤šå…ƒåŒ–ï¼Œå…±æ”¶å½• {len(articles)} æ¡æ–°é—»ã€‚"

        return trend_summary, hot_keywords


def main():
    """æµ‹è¯•å…¥å£"""
    from dotenv import load_dotenv
    load_dotenv()

    # Mock æµ‹è¯•æ•°æ®
    test_articles = [
        Article(
            title='AIæ—¶ä»£çš„å…¨çƒåˆ›ä½œæ¶ˆè´¹å¹³å°ï¼Œå‡ºç°äº†ä¸€å®¶æ¥è‡ªä¸­å›½çš„"éšå½¢å† å†›"',
            summary="SeaArt æœˆæ´»ç”¨æˆ·çªç ´2500ä¸‡ï¼Œå¹´åº¦ç»å¸¸æ€§æ”¶å…¥è¶…è¿‡5000ä¸‡ç¾å…ƒ...",
            link="https://36kr.com/p/3638895618739335",
            source="36æ°ª",
            category="ç§‘æŠ€",
            published=datetime(2026, 1, 15, 10, 42, 30)
        ),
        Article(
            title="ç¬¬äºŒæ¬¡å¤–å–å¤§æˆ˜ï¼šå¹³å°èµ„æœ¬å’Œå›½å®¶ç›‘ç®¡çš„åšå¼ˆï¼Œè°è¾“è°èµ¢ï¼Ÿ",
            summary="ç¾å›¢ã€æ·˜å®ã€äº¬ä¸œçš„å¤–å–å¤§æˆ˜ï¼Œæˆ‘ä»¬æ™®é€šäººçœ‹åˆ°çš„æ˜¯ä¸‰å¤§å¹³å°äº’æŠ¢è®¢å•ã€ç‹‚æ’’çº¢åŒ…...",
            link="http://www.huxiu.com/article/4826742.html",
            source="è™å—…",
            category="å•†ä¸š",
            published=datetime(2026, 1, 15, 10, 32, 58)
        )
    ]

    writer = XiaohongshuGuideWriter()
    guide = writer.generate_guide(test_articles, datetime.now())

    output_path = "output/test/xiaohongshu-guide-test.md"
    saved_path = guide.save_to_markdown(output_path)

    print(f"\nâœ… æµ‹è¯•æˆåŠŸï¼æŒ‡å—å·²ä¿å­˜åˆ°: {saved_path}")
    print(f"ğŸ“Š é«˜é€‚é…: {len(guide.high_priority)}, ä¸­é€‚é…: {len(guide.medium_priority)}, ä½é€‚é…: {len(guide.low_priority)}")


if __name__ == "__main__":
    main()
