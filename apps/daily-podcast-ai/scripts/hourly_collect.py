#!/usr/bin/env python3
"""
æ¯å°æ—¶æ–°é—»æ”¶é›†è„šæœ¬
å®šæ—¶ä»RSSæºè·å–æ–°é—»å¹¶å­˜å‚¨åˆ°ç¼“å­˜ï¼Œä¾›æ—©ä¸Š7ç‚¹ä¼˜é€‰ä½¿ç”¨
"""

import json
import sys
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / "src"))

from dotenv import load_dotenv
from news_sources import RSSFetcher

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv(project_root / ".env")


def load_cache(cache_path: Path) -> list[dict]:
    """åŠ è½½ç¼“å­˜çš„æ–°é—»"""
    if cache_path.exists():
        with open(cache_path, "r", encoding="utf-8") as f:
            return json.load(f)
    return []


def save_cache(cache_path: Path, news_list: list[dict]):
    """ä¿å­˜æ–°é—»åˆ°ç¼“å­˜"""
    cache_path.parent.mkdir(parents=True, exist_ok=True)
    with open(cache_path, "w", encoding="utf-8") as f:
        json.dump(news_list, f, ensure_ascii=False, indent=2)


def article_to_dict(article) -> dict:
    """å°† Article å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸"""
    return {
        "title": article.title,
        "summary": article.summary,
        "link": article.link,
        "source": article.source,
        "category": article.category,
        "published": article.published.isoformat() if article.published else None,
        "collected_at": datetime.now().isoformat()
    }


def is_duplicate(article_dict: dict, existing_news: list[dict]) -> bool:
    """æ£€æŸ¥æ˜¯å¦é‡å¤"""
    # åŸºäº link æˆ– title å»é‡
    for existing in existing_news:
        if existing["link"] == article_dict["link"]:
            return True
        if existing["title"] == article_dict["title"]:
            return True
    return False


def main():
    """ä¸»å…¥å£"""
    today = datetime.now().strftime("%Y-%m-%d")
    cache_dir = project_root / "cache"
    cache_path = cache_dir / f"{today}-news.json"

    current_hour = datetime.now().strftime("%H:%M")

    print(f"â° [{current_hour}] å¼€å§‹æ”¶é›†æ–°é—»...")

    # åŠ è½½ç°æœ‰ç¼“å­˜
    existing_news = load_cache(cache_path)
    print(f"ğŸ“‚ å½“å‰ç¼“å­˜: {len(existing_news)} ç¯‡")

    # è·å–æ–°é—»
    fetcher = RSSFetcher()
    articles = fetcher.fetch_all()

    if not articles:
        print("âŒ æœªè·å–åˆ°æ–°é—»")
        sys.exit(1)

    # å»é‡å¹¶è¿½åŠ 
    new_count = 0
    for article in articles:
        article_dict = article_to_dict(article)
        if not is_duplicate(article_dict, existing_news):
            existing_news.append(article_dict)
            new_count += 1

    # ä¿å­˜ç¼“å­˜
    save_cache(cache_path, existing_news)

    print(f"âœ… æ–°å¢ {new_count} ç¯‡ï¼Œæ€»è®¡ {len(existing_news)} ç¯‡")
    print(f"ğŸ’¾ å·²ä¿å­˜åˆ°: {cache_path}")


if __name__ == "__main__":
    main()
