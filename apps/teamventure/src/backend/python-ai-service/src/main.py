"""
TeamVenture AI Service - ä¸»åº”ç”¨å…¥å£

æŠ€æœ¯æ ˆï¼š
- FastAPI 0.109+
- LangGraph 0.0.40+
- OpenAI GPT-4
- Redis (ç¼“å­˜)
- RabbitMQ (æ¶ˆæ¯é˜Ÿåˆ—)

ä¸»è¦åŠŸèƒ½ï¼š
- æ¥æ”¶æ–¹æ¡ˆç”Ÿæˆè¯·æ±‚ï¼ˆMQæ¶ˆè´¹ï¼‰
- å¤šAgentåä½œç”Ÿæˆå›¢å»ºæ–¹æ¡ˆ
- å›è°ƒJavaæœåŠ¡å†™å…¥ç»“æœ

@author TeamVenture Team
@version 1.0.0
@since 2025-12-30
"""

import logging
from contextlib import asynccontextmanager

from fastapi import FastAPI
from fastapi import HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from prometheus_fastapi_instrumentator import Instrumentator

from src.models.config import settings
from src.langgraph.workflow import run_generation_workflow
from src.scheduler.scheduler import start_scheduler, stop_scheduler
from src.services.mq_consumer import start_mq_consumer, stop_mq_consumer
from src.services.markdown_converter import MarkdownConverter
from src.services.markdown_optimizer import MarkdownOptimizer
from src.services.xhs_normalizer import XhsNormalizer

# Import and initialize LLM metrics with Prometheus REGISTRY
from src.utils.llm_metrics import init_metrics as init_llm_metrics

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s - %(message)s",
    handlers=[logging.StreamHandler()],
)

logger = logging.getLogger(__name__)

# Avoid leaking query params (e.g., AMAP_API_KEY) via httpx INFO logs.
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    logger.info("ğŸš€ Starting TeamVenture AI Service...")

    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    try:
        # Initialize LLM metrics (so they appear in /metrics even before first call)
        init_llm_metrics(default_model=settings.openai_model)
        logger.info("âœ… LLM metrics initialized")

        # å¯åŠ¨MQæ¶ˆè´¹è€…
        await start_mq_consumer()
        logger.info("âœ… MQ Consumer started")

        # å¯åŠ¨å®šæ—¶ä»»åŠ¡è°ƒåº¦å™¨
        await start_scheduler()
        logger.info("âœ… Scheduler started")

        logger.info(f"ğŸ¯ AI Service running on: {settings.host}:{settings.port}")
        logger.info(f"ğŸ“š API Docs: http://{settings.host}:{settings.port}/docs")
        logger.info(f"ğŸ’š Health Check: http://{settings.host}:{settings.port}/health")

    except Exception as e:
        logger.error(f"âŒ Startup failed: {e}")
        raise

    yield

    # å…³é—­æ—¶æ¸…ç†
    logger.info("ğŸ›‘ Shutting down TeamVenture AI Service...")
    try:
        await stop_mq_consumer()
        logger.info("âœ… MQ Consumer stopped")

        await stop_scheduler()
        logger.info("âœ… Scheduler stopped")
    except Exception as e:
        logger.error(f"âš ï¸ Shutdown warning: {e}")


# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="TeamVenture AI Service",
    description="å›¢å»ºæ–¹æ¡ˆæ™ºèƒ½ç”ŸæˆæœåŠ¡ - AIé©±åŠ¨çš„å¤šAgentåä½œç³»ç»Ÿ",
    version="1.0.0",
    lifespan=lifespan,
    docs_url="/docs",
    redoc_url="/redoc",
)

# Instrument the app with default metrics
Instrumentator().instrument(app).expose(app)


# CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒåº”é™åˆ¶å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)


# ==================== å¥åº·æ£€æŸ¥ ====================
@app.get("/health", tags=["Health"])
async def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return JSONResponse(
        status_code=200,
        content={
            "status": "healthy",
            "service": "teamventure-ai-service",
            "version": "1.0.0",
        },
    )


@app.get("/", tags=["Root"])
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "Welcome to TeamVenture AI Service",
        "docs": "/docs",
        "health": "/health",
    }


# ==================== APIè·¯ç”± ====================
# æ³¨æ„ï¼šä¸€æœŸä¸»è¦é€šè¿‡MQæ¶ˆè´¹ï¼ŒHTTPæ¥å£ä½œä¸ºå¤‡ç”¨/è°ƒè¯•
@app.post("/api/v1/plans/generate", tags=["Plans"])
async def generate_plan_http(request: dict):
    """
    HTTPæ–¹å¼åŒæ­¥ç”Ÿæˆæ–¹æ¡ˆï¼ˆå¤‡ç”¨/è°ƒè¯•/åŒæ­¥é“¾è·¯ï¼‰

    è¯´æ˜ï¼š
    - è¯¥æ¥å£ä¼šåŒæ­¥è°ƒç”¨ LangGraph å·¥ä½œæµç”Ÿæˆæ–¹æ¡ˆï¼Œå¹¶ç›´æ¥è¿”å› plans åˆ—è¡¨ã€‚
    - ç”Ÿäº§ç¯å¢ƒä»å¯ç»§ç»­ä½¿ç”¨MQå¼‚æ­¥æ–¹å¼ï¼›Javaä¾§å¦‚éœ€åŒæ­¥é“¾è·¯ï¼Œå¯è°ƒç”¨æ­¤æ¥å£ã€‚
    """
    payload = request or {}
    state = await run_generation_workflow(payload)
    if state.get("error"):
        return JSONResponse(
            status_code=500,
            content={
                "error": state.get("error") or "generation failed",
                "plan_request_id": payload.get("plan_request_id"),
            },
        )

    return JSONResponse(
        status_code=200,
        content={
            "plan_request_id": state.get("plan_request_id"),
            "user_id": state.get("user_id"),
            "plans": state.get("generated_plans", []),
            "trace_id": payload.get("trace_id"),
        },
    )


class XhsNormalizeRequest(BaseModel):
    url: str = Field(default="", description="XHS share URL")
    title: str = Field(default="", description="Extracted title")
    extracted_text: str = Field(default="", description="Extracted note text")
    model: str | None = Field(default=None, description="Optional OpenAI model override (e.g. gpt-5.2)")


class XhsNormalizeResponse(BaseModel):
    content: str


@app.post("/api/v1/import/xiaohongshu/normalize", tags=["Import"])
async def normalize_xhs_text(req: XhsNormalizeRequest):
    """
    Normalize XHS extracted text via GPT (two-stage flow).

    Returns plain text content only; if OPENAI_API_KEY is missing, returns extracted_text as-is.
    """
    normalizer = XhsNormalizer()
    content = await normalizer.normalize_original_text(
        url=req.url,
        title=req.title,
        extracted_text=req.extracted_text,
        model=req.model,
    )
    return XhsNormalizeResponse(content=content)


class MarkdownOptimizeRequest(BaseModel):
    markdown_content: str = Field(default="", description="Markdown draft to optimize")
    model: str | None = Field(default=None, description="Optional OpenAI model override (e.g. gpt-5.2)")


class MarkdownOptimizeResponse(BaseModel):
    markdown_content: str


@app.post("/api/v1/markdown/optimize", tags=["Markdown"])
async def optimize_markdown(req: MarkdownOptimizeRequest):
    """
    Optimize markdown formatting via GPT.

    If OPENAI_API_KEY is missing, returns markdown_content as-is.
    """
    optimizer = MarkdownOptimizer()
    content = await optimizer.optimize_markdown(markdown_content=req.markdown_content, model=req.model)
    return MarkdownOptimizeResponse(markdown_content=content)


class MarkdownConvertRequest(BaseModel):
    parsed_content: str = Field(default="", description="Plain text parsed from XHS")
    model: str | None = Field(default=None, description="Optional OpenAI model override (e.g. gpt-5.2)")


class MarkdownConvertResponse(BaseModel):
    markdown_content: str


@app.post("/api/v1/markdown/convert", tags=["Markdown"])
async def convert_to_markdown(req: MarkdownConvertRequest):
    """
    Convert parsed plain text into markdown via GPT.

    This endpoint requires OPENAI_API_KEY; conversion is done by the LLM to handle free-form user text.
    """
    converter = MarkdownConverter()
    try:
        content = await converter.convert_parsed_text_to_markdown(parsed_content=req.parsed_content, model=req.model)
        return MarkdownConvertResponse(markdown_content=content)
    except RuntimeError as e:
        msg = str(e) or "Markdown convert failed"
        if "OPENAI_API_KEY" in msg:
            raise HTTPException(status_code=503, detail="AI is not configured (missing OPENAI_API_KEY)")
        raise HTTPException(status_code=502, detail=msg)


if __name__ == "__main__":
    import uvicorn

    uvicorn.run(
        "src.main:app",
        host=settings.host,
        port=settings.port,
        reload=settings.debug,
        log_level="info",
    )
