"""
æ–°æ¦œå°çº¢ä¹¦æ•°æ®æŠ“å–å™¨
æŠ“å– https://xh.newrank.cn/content/topicRank/hotTopic
"""
import re
import time
import yaml
from pathlib import Path
from typing import Optional
from datetime import datetime

from playwright.async_api import Page, Locator

from .browser_manager import BrowserManager
from ..models.topic import XHSTopic
from ..utils.logger import get_logger

logger = get_logger()


class SelectorConfig:
    """é€‰æ‹©å™¨é…ç½®ç®¡ç†"""

    def __init__(self, config_path: str = "config/scraper.yaml"):
        """
        åˆå§‹åŒ–

        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        config_file = Path(config_path)
        if not config_file.exists():
            raise FileNotFoundError(f"Config file not found: {config_path}")

        with open(config_file, "r", encoding="utf-8") as f:
            self.config = yaml.safe_load(f)

        self.newrank = self.config.get("newrank", {})
        self.selectors = self.newrank.get("selectors", {})
        self.wait_config = self.newrank.get("wait", {})
        self.scroll_config = self.newrank.get("scroll", {})
        self.request_config = self.newrank.get("request", {})
        self.filters = self.config.get("filters", {})

    @property
    def base_url(self) -> str:
        """è·å–åŸºç¡€URL"""
        return self.newrank.get("base_url", "")

    @property
    def timeout(self) -> int:
        """è·å–è¶…æ—¶æ—¶é—´"""
        return self.wait_config.get("timeout", 15000)

    @property
    def scroll_enabled(self) -> bool:
        """æ˜¯å¦å¯ç”¨æ»šåŠ¨"""
        return self.scroll_config.get("enabled", True)

    @property
    def max_scrolls(self) -> int:
        """æœ€å¤§æ»šåŠ¨æ¬¡æ•°"""
        return self.scroll_config.get("max_scrolls", 5)

    @property
    def scroll_pause_ms(self) -> int:
        """æ»šåŠ¨æš‚åœæ—¶é—´ï¼ˆæ¯«ç§’ï¼‰"""
        return self.scroll_config.get("pause_ms", 2000)

    @property
    def min_heat_score(self) -> int:
        """æœ€å°çƒ­åº¦å€¼"""
        return self.filters.get("min_heat_score", 1000)

    @property
    def max_topics(self) -> int:
        """æœ€å¤§è¯é¢˜æ•°"""
        return self.filters.get("max_topics", 50)


class NewRankScraper:
    """æ–°æ¦œå°çº¢ä¹¦æ•°æ®æŠ“å–å™¨"""

    def __init__(
        self, browser_manager: BrowserManager, config_path: str = "config/scraper.yaml"
    ):
        """
        åˆå§‹åŒ–

        Args:
            browser_manager: æµè§ˆå™¨ç®¡ç†å™¨
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        self.browser = browser_manager
        self.config = SelectorConfig(config_path)

    async def scrape_hot_topics(self, limit: int = 50) -> list[XHSTopic]:
        """
        æŠ“å–çƒ­é—¨è¯é¢˜

        Args:
            limit: æœ€å¤§è¯é¢˜æ•°é‡

        Returns:
            è¯é¢˜åˆ—è¡¨
        """
        logger.info(f"å¼€å§‹æŠ“å–æ–°æ¦œå°çº¢ä¹¦çƒ­é—¨è¯é¢˜ï¼Œç›®æ ‡æ•°é‡: {limit}")

        page = await self.browser.new_page()

        try:
            # 1. å¯¼èˆªåˆ°ç›®æ ‡é¡µé¢
            logger.info(f"å¯¼èˆªåˆ°: {self.config.base_url}")
            await page.goto(
                self.config.base_url, timeout=self.config.request_config.get("timeout", 30000)
            )

            # 2. ç­‰å¾…å†…å®¹åŠ è½½
            logger.info("ç­‰å¾…å†…å®¹åŠ è½½...")
            await self._wait_for_content(page)

            # 3. æ»šåŠ¨åŠ è½½æ›´å¤šï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if self.config.scroll_enabled:
                logger.info("æ»šåŠ¨åŠ è½½æ›´å¤šæ•°æ®...")
                await self._scroll_to_load_more(page)

            # 4. æå–è¯é¢˜æ•°æ®
            logger.info("å¼€å§‹æå–è¯é¢˜æ•°æ®...")
            topics = await self._extract_all_topics(page, limit)

            logger.info(f"æŠ“å–å®Œæˆï¼Œå…±è·å– {len(topics)} ä¸ªè¯é¢˜")
            return topics

        except Exception as e:
            logger.error(f"æŠ“å–å¤±è´¥: {e}")
            # æˆªå›¾ä¿å­˜ï¼ˆç”¨äºè°ƒè¯•ï¼‰
            screenshot_path = f"logs/error-screenshot-{int(time.time())}.png"
            await page.screenshot(path=screenshot_path)
            logger.info(f"é”™è¯¯æˆªå›¾å·²ä¿å­˜: {screenshot_path}")
            raise

        finally:
            await page.close()

    async def _wait_for_content(self, page: Page):
        """
        ç­‰å¾…å†…å®¹åŠ è½½ï¼ˆå¤šé€‰æ‹©å™¨å®¹é”™ï¼‰

        Args:
            page: Playwrighté¡µé¢å®ä¾‹

        Raises:
            TimeoutError: æ‰€æœ‰é€‰æ‹©å™¨éƒ½è¶…æ—¶
        """
        selectors = self.config.selectors.get("topic_list", [])

        for selector in selectors:
            try:
                logger.info(f"  å°è¯•é€‰æ‹©å™¨: {selector}")
                await page.wait_for_selector(selector, timeout=self.config.timeout)
                logger.info(f"  âœ… é€‰æ‹©å™¨åŒ¹é…æˆåŠŸ: {selector}")
                return
            except Exception as e:
                logger.info(f"  é€‰æ‹©å™¨å¤±è´¥: {selector}")
                continue

        raise TimeoutError("æ— æ³•æ‰¾åˆ°è¯é¢˜åˆ—è¡¨å®¹å™¨ï¼Œæ‰€æœ‰é€‰æ‹©å™¨å‡å¤±è´¥")

    async def _scroll_to_load_more(self, page: Page):
        """
        æ»šåŠ¨é¡µé¢åŠ è½½æ›´å¤šå†…å®¹

        Args:
            page: Playwrighté¡µé¢å®ä¾‹
        """
        for i in range(self.config.max_scrolls):
            logger.info(f"  æ»šåŠ¨ {i + 1}/{self.config.max_scrolls}")

            # æ»šåŠ¨åˆ°åº•éƒ¨
            await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")

            # ç­‰å¾…åŠ è½½
            await page.wait_for_timeout(self.config.scroll_pause_ms)

    async def _extract_all_topics(self, page: Page, limit: int) -> list[XHSTopic]:
        """
        æå–æ‰€æœ‰è¯é¢˜æ•°æ®

        Args:
            page: Playwrighté¡µé¢å®ä¾‹
            limit: æœ€å¤§æ•°é‡

        Returns:
            è¯é¢˜åˆ—è¡¨
        """
        topics = []

        # æŸ¥æ‰¾æ‰€æœ‰è¯é¢˜é¡¹
        topic_items = await self._find_topic_items(page)
        logger.info(f"  æ‰¾åˆ° {len(topic_items)} ä¸ªè¯é¢˜é¡¹")

        for i, item in enumerate(topic_items[:limit]):
            try:
                topic = await self._extract_topic_from_element(item, i + 1)
                if topic:
                    topics.append(topic)
                    logger.info(f"  [{i + 1}/{limit}] {topic.title} - çƒ­åº¦: {topic.heat_score_formatted}")
            except Exception as e:
                logger.error(f"  æå–è¯é¢˜ {i + 1} å¤±è´¥: {e}")
                continue

        return topics

    async def _find_topic_items(self, page: Page) -> list[Locator]:
        """
        æŸ¥æ‰¾æ‰€æœ‰è¯é¢˜é¡¹å…ƒç´ 

        Args:
            page: Playwrighté¡µé¢å®ä¾‹

        Returns:
            è¯é¢˜å…ƒç´ åˆ—è¡¨
        """
        selectors = self.config.selectors.get("topic_item", [])

        for selector in selectors:
            try:
                items = page.locator(selector)
                count = await items.count()
                if count > 0:
                    logger.info(f"  ä½¿ç”¨é€‰æ‹©å™¨ {selector}ï¼Œæ‰¾åˆ° {count} ä¸ªå…ƒç´ ")
                    return [items.nth(i) for i in range(count)]
            except Exception:
                continue

        logger.warning("  æœªæ‰¾åˆ°è¯é¢˜é¡¹ï¼Œè¿”å›ç©ºåˆ—è¡¨")
        return []

    async def _extract_topic_from_element(
        self, element: Locator, index: int
    ) -> Optional[XHSTopic]:
        """
        ä»DOMå…ƒç´ æå–è¯é¢˜æ•°æ®

        Args:
            element: è¯é¢˜é¡¹å…ƒç´ 
            index: ç´¢å¼•ï¼ˆç”¨ä½œæ’åï¼‰

        Returns:
            è¯é¢˜å¯¹è±¡ï¼Œæå–å¤±è´¥è¿”å›None
        """
        try:
            # æå–å„å­—æ®µ
            title = await self._try_extract_text(element, "title")
            if not title:
                return None  # æ ‡é¢˜ä¸ºå¿…éœ€å­—æ®µ

            heat_score = await self._try_extract_number(element, "heat_score")
            category = await self._try_extract_text(element, "category")
            trend = await self._extract_trend(element)

            # ç”Ÿæˆå”¯ä¸€ID
            topic_id = f"topic_{index}_{int(time.time())}"

            return XHSTopic(
                topic_id=topic_id,
                title=title.strip(),
                heat_score=heat_score,
                category=category or "æœªåˆ†ç±»",
                rank=index,
                trend_direction=trend["direction"],
                trend_icon=trend["icon"],
                rank_change=trend["change"],
                crawled_at=datetime.now(),
            )

        except Exception as e:
            logger.error(f"  æå–è¯é¢˜æ•°æ®å¤±è´¥ (index={index}): {e}")
            return None

    async def _try_extract_text(
        self, element: Locator, field_name: str
    ) -> str:
        """
        å°è¯•å¤šä¸ªé€‰æ‹©å™¨æå–æ–‡æœ¬

        Args:
            element: çˆ¶å…ƒç´ 
            field_name: å­—æ®µåï¼ˆåœ¨configä¸­çš„keyï¼‰

        Returns:
            æå–çš„æ–‡æœ¬ï¼Œå¤±è´¥è¿”å›ç©ºå­—ç¬¦ä¸²
        """
        selectors = self.config.selectors.get(field_name, [])

        for selector in selectors:
            try:
                el = element.locator(selector).first
                if await el.count() > 0:
                    text = await el.text_content()
                    if text and text.strip():
                        return text.strip()
            except Exception:
                continue

        return ""

    async def _try_extract_number(
        self, element: Locator, field_name: str
    ) -> int:
        """
        å°è¯•æå–æ•°å€¼ï¼ˆæ”¯æŒ"ä¸‡"ã€"äº¿"å•ä½ï¼‰

        Args:
            element: çˆ¶å…ƒç´ 
            field_name: å­—æ®µå

        Returns:
            æå–çš„æ•°å€¼ï¼Œå¤±è´¥è¿”å›0
        """
        text = await self._try_extract_text(element, field_name)
        if not text:
            return 0

        return self._parse_number(text)

    def _parse_number(self, text: str) -> int:
        """
        è§£ææ•°å€¼å­—ç¬¦ä¸²ï¼ˆæ”¯æŒä¸­æ–‡å•ä½ï¼‰

        Args:
            text: æ•°å€¼å­—ç¬¦ä¸²ï¼Œå¦‚"12.5ä¸‡"ã€"1.2äº¿"

        Returns:
            æ•´æ•°å€¼
        """
        text = text.strip()

        # ç§»é™¤é€—å·
        text = text.replace(",", "")

        # åŒ¹é…æ•°å­—
        match = re.search(r"([\d.]+)\s*([ä¸‡äº¿]?)", text)
        if not match:
            return 0

        num_str, unit = match.groups()

        try:
            num = float(num_str)
        except ValueError:
            return 0

        # å•ä½è½¬æ¢
        if unit == "ä¸‡":
            return int(num * 10_000)
        elif unit == "äº¿":
            return int(num * 100_000_000)
        else:
            return int(num)

    async def _extract_trend(self, element: Locator) -> dict:
        """
        æå–è¶‹åŠ¿ä¿¡æ¯

        Args:
            element: è¯é¢˜é¡¹å…ƒç´ 

        Returns:
            è¶‹åŠ¿ä¿¡æ¯å­—å…¸ {"direction": "up/down/stable", "icon": "â†‘/â†“/â†’", "change": 0}
        """
        trend_text = await self._try_extract_text(element, "trend")

        if not trend_text:
            return {"direction": "stable", "icon": "â†’", "change": 0}

        # æ£€æµ‹è¶‹åŠ¿æ–¹å‘
        if "â†‘" in trend_text or "up" in trend_text.lower() or "ä¸Šå‡" in trend_text:
            direction = "up"
            icon = "â†‘"
        elif "â†“" in trend_text or "down" in trend_text.lower() or "ä¸‹é™" in trend_text:
            direction = "down"
            icon = "â†“"
        elif "new" in trend_text.lower() or "æ–°" in trend_text:
            direction = "new"
            icon = "ğŸ†•"
        else:
            direction = "stable"
            icon = "â†’"

        # æå–å˜åŒ–æ•°å€¼
        change_match = re.search(r"(\d+)", trend_text)
        change = int(change_match.group(1)) if change_match else 0

        # ä¸‹é™è¶‹åŠ¿çš„å˜åŒ–å€¼ä¸ºè´Ÿæ•°
        if direction == "down":
            change = -change

        return {"direction": direction, "icon": icon, "change": change}

    async def scrape_with_retry(
        self, max_retries: int = 3, limit: int = 50
    ) -> list[XHSTopic]:
        """
        å¸¦é‡è¯•çš„æŠ“å–

        Args:
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            limit: æœ€å¤§è¯é¢˜æ•°

        Returns:
            è¯é¢˜åˆ—è¡¨

        Raises:
            Exception: é‡è¯•å¤±è´¥åæŠ›å‡ºå¼‚å¸¸
        """
        for attempt in range(max_retries):
            try:
                return await self.scrape_hot_topics(limit)
            except Exception as e:
                logger.warning(f"æŠ“å–å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 5
                    logger.info(f"ç­‰å¾… {wait_time} ç§’åé‡è¯•...")
                    time.sleep(wait_time)
                else:
                    raise
