"""
æ’­å®¢å¯¹è¯è„šæœ¬ç”Ÿæˆå™¨
ä½¿ç”¨ Gemini 2.5 Pro å°†å°çº¢ä¹¦è¯é¢˜åˆ†æè½¬åŒ–ä¸ºåŒäººå¯¹è°ˆè„šæœ¬
"""
import json
import os
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import Optional

import google.generativeai as genai

from ..models.topic import TopicAnalysisResult, AIInsight, XHSTopic
from ..utils.logger import get_logger

logger = get_logger()


@dataclass
class DialogueLine:
    """å¯¹è¯è¡Œ"""

    speaker: str  # "å°é›…" or "æ¤èŒ"
    text: str  # å¯¹è¯å†…å®¹
    emotion: str = "neutral"  # æƒ…ç»ª: excited, thoughtful, curious, neutral

    def to_dict(self) -> dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)


@dataclass
class PodcastScript:
    """æ’­å®¢è„šæœ¬"""

    title: str  # æ’­å®¢æ ‡é¢˜
    date: str  # æ—¥æœŸ
    episode_number: Optional[int] = None  # é›†æ•°ï¼ˆå¯é€‰ï¼‰
    lines: list[DialogueLine] = None  # å¯¹è¯è¡Œåˆ—è¡¨
    duration_estimate: int = 600  # é¢„è®¡æ—¶é•¿ï¼ˆç§’ï¼‰

    def __post_init__(self):
        if self.lines is None:
            self.lines = []

    def to_json(self) -> str:
        """è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²"""
        data = {
            "title": self.title,
            "date": self.date,
            "episode_number": self.episode_number,
            "duration_estimate": self.duration_estimate,
            "lines": [line.to_dict() for line in self.lines],
        }
        return json.dumps(data, ensure_ascii=False, indent=2)

    def save_to_file(self, output_dir: Path) -> tuple[Path, Path]:
        """
        ä¿å­˜è„šæœ¬åˆ°æ–‡ä»¶

        Args:
            output_dir: è¾“å‡ºç›®å½•

        Returns:
            (JSONè·¯å¾„, Markdownè·¯å¾„)
        """
        output_dir = Path(output_dir)
        output_dir.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜JSONï¼ˆç”¨äºTTSå¤„ç†ï¼‰
        json_path = output_dir / f"script-{self.date}.json"
        json_path.write_text(self.to_json(), encoding="utf-8")

        # ä¿å­˜Markdownï¼ˆä¾¿äºäººç±»é˜…è¯»ï¼‰
        md_path = output_dir / f"script-{self.date}.md"
        md_content = self._to_markdown()
        md_path.write_text(md_content, encoding="utf-8")

        return json_path, md_path

    def _to_markdown(self) -> str:
        """è½¬æ¢ä¸ºMarkdownæ ¼å¼"""
        lines_md = []

        # æ ‡é¢˜
        lines_md.append(f"# {self.title}\n")
        lines_md.append(f"**æ—¥æœŸ**: {self.date}")

        if self.episode_number:
            lines_md.append(f"**ç¬¬{self.episode_number}æœŸ**")

        lines_md.append(f"**é¢„è®¡æ—¶é•¿**: {self.duration_estimate // 60}åˆ†é’Ÿ\n")
        lines_md.append("---\n")

        # å¯¹è¯å†…å®¹
        for line in self.lines:
            # ä½¿ç”¨emojiåŒºåˆ†è¯´è¯è€…
            speaker_icon = "ğŸ™ï¸" if line.speaker == "å°é›…" else "ğŸ’¡"
            emotion_tag = f" ({line.emotion})" if line.emotion != "neutral" else ""

            lines_md.append(
                f"**{speaker_icon} {line.speaker}**{emotion_tag}: {line.text}\n"
            )

        # é¡µè„š
        lines_md.append("\n---\n")
        lines_md.append(
            f"*è‡ªåŠ¨ç”Ÿæˆäº {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*"
        )

        return "\n".join(lines_md)


class DialogueWriter:
    """æ’­å®¢è„šæœ¬ç”Ÿæˆå™¨ï¼ˆä½¿ç”¨Gemini APIï¼‰"""

    def __init__(
        self,
        model_name: str = "gemini-2.0-flash-exp",
        temperature: float = 0.9,
        max_output_tokens: int = 8192,
    ):
        """
        åˆå§‹åŒ–

        Args:
            model_name: Geminiæ¨¡å‹åç§°
            temperature: ç”Ÿæˆæ¸©åº¦ï¼ˆ0-1ï¼Œè¶Šé«˜è¶Šéšæœºï¼‰
            max_output_tokens: æœ€å¤§è¾“å‡ºtokenæ•°
        """
        # ä»ç¯å¢ƒå˜é‡è¯»å–API Key
        api_key = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("éœ€è¦è®¾ç½® GOOGLE_API_KEY æˆ– GEMINI_API_KEY ç¯å¢ƒå˜é‡")

        genai.configure(api_key=api_key)
        self.model = genai.GenerativeModel(model_name)
        self.temperature = temperature
        self.max_output_tokens = max_output_tokens

        logger.info(f"DialogueWriter åˆå§‹åŒ–å®Œæˆ (model={model_name})")

    def generate(
        self,
        analysis_result: TopicAnalysisResult,
        ai_insight: Optional[AIInsight] = None,
        target_duration: int = 600,
    ) -> PodcastScript:
        """
        ç”Ÿæˆæ’­å®¢å¯¹è¯è„šæœ¬

        Args:
            analysis_result: è¯é¢˜åˆ†æç»“æœ
            ai_insight: AIæ´å¯Ÿï¼ˆå¯é€‰ï¼‰
            target_duration: ç›®æ ‡æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤10åˆ†é’Ÿ

        Returns:
            æ’­å®¢è„šæœ¬å¯¹è±¡
        """
        logger.info("å¼€å§‹ç”Ÿæˆæ’­å®¢å¯¹è¯è„šæœ¬...")

        # 1. å‡†å¤‡è¾“å…¥ä¸Šä¸‹æ–‡
        context = self._build_context(analysis_result, ai_insight)

        # 2. æ„å»ºPrompt
        prompt = self._build_prompt(analysis_result, target_duration)

        # 3. è°ƒç”¨Gemini API
        try:
            logger.info("  è°ƒç”¨ Gemini API...")
            response = self.model.generate_content(
                [prompt, context],
                generation_config={
                    "temperature": self.temperature,
                    "max_output_tokens": self.max_output_tokens,
                    "response_mime_type": "application/json",
                },
            )

            # 4. è§£æç»“æœ
            dialogue_data = json.loads(response.text)
            lines = [
                DialogueLine(
                    speaker=item.get("speaker", "å°é›…"),
                    text=item.get("text", ""),
                    emotion=item.get("emotion", "neutral"),
                )
                for item in dialogue_data
            ]

            script = PodcastScript(
                title=f"å°çº¢ä¹¦çƒ­é—¨è¯é¢˜ç ”ç©¶ - {analysis_result.date}",
                date=analysis_result.date,
                lines=lines,
                duration_estimate=target_duration,
            )

            logger.info(f"  âœ“ è„šæœ¬ç”Ÿæˆå®Œæˆï¼Œå…±{len(lines)}è¡Œå¯¹è¯")
            return script

        except json.JSONDecodeError as e:
            logger.error(f"  âœ— JSONè§£æå¤±è´¥: {e}")
            logger.error(f"  åŸå§‹è¾“å‡º: {response.text[:500]}...")
            # è¿”å›é”™è¯¯è„šæœ¬
            return self._create_error_script(analysis_result.date)

        except Exception as e:
            logger.error(f"  âœ— è„šæœ¬ç”Ÿæˆå¤±è´¥: {e}")
            return self._create_error_script(analysis_result.date)

    def _build_context(
        self, analysis_result: TopicAnalysisResult, ai_insight: Optional[AIInsight]
    ) -> str:
        """æ„å»ºè¾“å…¥ä¸Šä¸‹æ–‡"""
        context_parts = []

        # åŸºç¡€ä¿¡æ¯
        context_parts.append(f"**åˆ†ææ—¥æœŸ**: {analysis_result.date}")
        context_parts.append(f"**è¯é¢˜æ€»æ•°**: {analysis_result.total_topics}")
        context_parts.append(
            f"**æ€»çƒ­åº¦**: {analysis_result.total_heat / 10000:.1f}ä¸‡\n"
        )

        # Topè¯é¢˜
        context_parts.append("## Top 10 çƒ­é—¨è¯é¢˜\n")
        for topic in analysis_result.top_topics[:10]:
            heat = topic.heat_score_formatted
            read = self._format_number(topic.read_count)
            notes = self._format_number(topic.note_count)
            trend = topic.rank_change_text

            context_parts.append(
                f"{topic.rank}. **{topic.title}** "
                f"(çƒ­åº¦: {heat}, é˜…è¯»: {read}, ç¬”è®°: {notes}, è¶‹åŠ¿: {trend})"
            )

        context_parts.append("")

        # çƒ­è¯
        if analysis_result.top_keywords:
            keywords_text = "ã€".join(analysis_result.top_keywords[:15])
            context_parts.append(f"## çƒ­è¯Top 15\n{keywords_text}\n")

        # åˆ†ç±»ç»Ÿè®¡
        if analysis_result.category_stats:
            context_parts.append("## åˆ†ç±»åˆ†å¸ƒ\n")
            for category, stats in list(analysis_result.category_stats.items())[:5]:
                count = stats.get("count", 0)
                context_parts.append(f"- {category}: {count}ä¸ªè¯é¢˜")
            context_parts.append("")

        # AIæ´å¯Ÿ
        if ai_insight:
            context_parts.append("## AIæ´å¯Ÿ\n")

            if ai_insight.user_behavior:
                context_parts.append("**ç”¨æˆ·è¡Œä¸º**:")
                for item in ai_insight.user_behavior[:3]:
                    context_parts.append(f"- {item}")
                context_parts.append("")

            if ai_insight.trend_predictions:
                context_parts.append("**è¶‹åŠ¿é¢„æµ‹**:")
                for item in ai_insight.trend_predictions[:3]:
                    context_parts.append(f"- {item}")
                context_parts.append("")

        return "\n".join(context_parts)

    def _build_prompt(
        self, analysis_result: TopicAnalysisResult, target_duration: int
    ) -> str:
        """æ„å»ºGemini Prompt"""
        target_minutes = target_duration // 60

        return f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ’­å®¢åˆ¶ä½œäººï¼Œè´Ÿè´£ä¸º"å°çº¢ä¹¦æ•°æ®è§‚å¯Ÿ"æ’­å®¢æ’°å†™å¯¹è¯è„šæœ¬ã€‚

## èŠ‚ç›®å®šä½
- **èŠ‚ç›®åç§°**: å°çº¢ä¹¦æ•°æ®è§‚å¯Ÿ
- **æ’­å‡ºé¢‘ç‡**: æ¯æ—¥
- **æ—¶é•¿**: {target_minutes}åˆ†é’Ÿå·¦å³
- **å—ä¼—**: å†…å®¹åˆ›ä½œè€…ã€å“ç‰Œæ–¹ã€æ•°æ®åˆ†æå¸ˆ

## ä¸»æŒäººè®¾å®š

**å°é›…** (å¥³æ€§ï¼Œæ•°æ®åˆ†æå¸ˆ):
- æ€§æ ¼ï¼šç†æ€§ã€ä¸“ä¸šã€å–„äºå‘ç°æ•°æ®èƒŒåçš„è§„å¾‹
- è§’è‰²ï¼šä¸»å¯¼æ•°æ®è§£è¯»ï¼Œæä¾›æ·±åº¦åˆ†æ
- è¯­è¨€é£æ ¼ï¼šç®€æ´ä¸“ä¸šï¼Œå–„ç”¨æ•°æ®å¯¹æ¯”
- å…¸å‹è¡¨è¾¾ï¼š"ä»æ•°æ®æ¥çœ‹..."ã€"è¿™ä¸ªè¶‹åŠ¿å€¼å¾—å…³æ³¨..."

**æ¤èŒ** (ç”·æ€§ï¼Œå†…å®¹åˆ›ä½œè€…):
- æ€§æ ¼ï¼šå¥½å¥‡ã€æ´»è·ƒã€ä»£è¡¨åˆ›ä½œè€…è§†è§’
- è§’è‰²ï¼šæå‡ºé—®é¢˜ï¼Œå…³æ³¨å®æ“å»ºè®®
- è¯­è¨€é£æ ¼ï¼šè½»æ¾æ´»æ³¼ï¼Œæ¥åœ°æ°”
- å…¸å‹è¡¨è¾¾ï¼š"è¿™ä¸ªæœ‰æ„æ€..."ã€"åˆ›ä½œè€…è¯¥æ€ä¹ˆåš..."

## è„šæœ¬è¦æ±‚

1. **è‡ªç„¶å¯¹è¯**ï¼š
   - ä½¿ç”¨å£è¯­åŒ–è¡¨è¾¾ï¼Œé¿å…ä¹¦é¢è¯­
   - åŠ å…¥è¯­æ°”è¯ï¼ˆ"è¯¶"ã€"å¯¹"ã€"å—¯"ã€"æ˜¯å§"ï¼‰
   - é€‚å½“æ‰“æ–­å’Œè¡¥å……ï¼ˆæ¨¡æ‹ŸçœŸå®å¯¹è¯ï¼‰

2. **ç»“æ„æ¸…æ™°**ï¼š
   - å¼€åœºï¼šç®€çŸ­æœ‰åŠ›çš„Hookï¼ˆ30ç§’å†…å¸å¼•æ³¨æ„ï¼‰
   - ä¸»ä½“ï¼š3-4ä¸ªè¯é¢˜æ¿å—ï¼Œæ¯ä¸ª2-3åˆ†é’Ÿ
   - ç»“å°¾ï¼šæ€»ç»“è¦ç‚¹ + è¡ŒåŠ¨å»ºè®®

3. **å†…å®¹æ·±åº¦**ï¼š
   - ä¸åªæ˜¯æ’­æŠ¥æ•°æ®ï¼Œè¦è§£é‡Š"ä¸ºä»€ä¹ˆ"
   - è¿æ¥ä¸åŒè¯é¢˜ï¼Œå‘ç°å…³è”
   - æä¾›å¯è½åœ°çš„åˆ›ä½œè€…å»ºè®®

4. **èŠ‚å¥æŠŠæ§**ï¼š
   - å¿«æ…¢ç»“åˆï¼šæ•°æ®éƒ¨åˆ†å¿«é€Ÿï¼Œæ´å¯Ÿéƒ¨åˆ†æ”¾æ…¢
   - é€‚æ—¶åˆ¶é€ æ‚¬å¿µï¼š"é‚£ä½ çŒœæ¥ä¸‹æ¥ä¼šæ€æ ·ï¼Ÿ"
   - é¿å…å•äººé•¿ç¯‡å¤§è®ºï¼ˆæ¯è½®å¯¹è¯æ§åˆ¶åœ¨50å­—å†…ï¼‰

5. **è¯­è¨€é£æ ¼**ï¼š
   - ä½¿ç”¨ä¸­æ–‡ï¼Œä¿ç•™"å°çº¢ä¹¦"ã€"äº’åŠ¨é‡"ç­‰å¹³å°æœ¯è¯­
   - æ•°å­—ç”¨ä¸­æ–‡è¡¨è¾¾ï¼š150ä¸‡ã€5000ä¸‡ï¼ˆä¸è¦ç”¨1.5Mï¼‰
   - é€‚å½“ä½¿ç”¨ç½‘ç»œæµè¡Œè¯­ï¼Œä½†ä¸è¿‡åº¦

## è¾“å‡ºæ ¼å¼ï¼ˆJSON Arrayï¼‰

```json
[
  {{"speaker": "å°é›…", "text": "...", "emotion": "excited"}},
  {{"speaker": "æ¤èŒ", "text": "...", "emotion": "curious"}}
]
```

**emotionå¯é€‰å€¼**: neutral, excited, thoughtful, curious, surprised, concerned

## è„šæœ¬ç¤ºä¾‹

å°é›…: å¤§å®¶å¥½ï¼Œæ¬¢è¿æ¥åˆ°å°çº¢ä¹¦æ•°æ®è§‚å¯Ÿï¼Œæˆ‘æ˜¯å°é›…ã€‚
æ¤èŒ: æˆ‘æ˜¯æ¤èŒã€‚ä»Šå¤©çš„æ•°æ®æœ‰ç‚¹æ„æ€å•Šï¼
å°é›…: å¯¹ï¼Œä»Šå¤©æŠ“å–äº†50ä¸ªçƒ­é—¨è¯é¢˜ï¼Œæ€»çƒ­åº¦è¾¾åˆ°5000ä¸‡ï¼Œæ¯”æ˜¨å¤©æ¶¨äº†15%ã€‚
æ¤èŒ: å“‡ï¼Œè¿™ä¸ªå¢é•¿å¹…åº¦ä¸å°ã€‚é‚£éƒ½æ˜¯å“ªäº›è¯é¢˜åœ¨å¸¦åŠ¨å¢é•¿å‘¢ï¼Ÿ
å°é›…: ä½ çœ‹æ’åç¬¬ä¸€çš„æ˜¯"æ˜¥èŠ‚å‡ºæ¸¸æ”»ç•¥"ï¼Œçƒ­åº¦150ä¸‡ï¼Œè€Œä¸”æ’åæ¯”æ˜¨å¤©ä¸Šå‡äº†2ä½ã€‚
æ¤èŒ: æ˜¥èŠ‚å˜›ï¼Œè¿™ä¸ªæ—¶é—´ç‚¹ç¡®å®æ˜¯å‡ºæ¸¸è§„åˆ’çš„é«˜å³°æœŸã€‚é‚£è¿™ä¸ªè¯é¢˜çš„æ•°æ®è¡¨ç°æ€ä¹ˆæ ·ï¼Ÿ
å°é›…: é˜…è¯»é‡5000ä¸‡ï¼Œç¬”è®°æ•°2ä¸‡ï¼Œäº’åŠ¨ç‡ç®—æ˜¯æ¯”è¾ƒé«˜çš„ã€‚

---

è¯·åŸºäºæä¾›çš„æ•°æ®ç”Ÿæˆå®Œæ•´çš„{target_minutes}åˆ†é’Ÿå¯¹è¯è„šæœ¬ã€‚
"""

    def _build_context(
        self, analysis_result: TopicAnalysisResult, ai_insight: Optional[AIInsight]
    ) -> str:
        """æ„å»ºæ•°æ®ä¸Šä¸‹æ–‡ï¼ˆå·²åœ¨ä¸Šé¢å®ç°ï¼‰"""
        # æ­¤æ–¹æ³•åœ¨ä¸Šé¢å·²å®ç°
        pass

    def _create_error_script(self, date: str) -> PodcastScript:
        """åˆ›å»ºé”™è¯¯å¤„ç†è„šæœ¬"""
        return PodcastScript(
            title=f"å°çº¢ä¹¦çƒ­é—¨è¯é¢˜ç ”ç©¶ - {date}",
            date=date,
            lines=[
                DialogueLine(
                    speaker="å°é›…",
                    text="æŠ±æ­‰ï¼Œä»Šå¤©çš„æ’­å®¢è„šæœ¬ç”Ÿæˆé‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ã€‚",
                    emotion="concerned",
                ),
                DialogueLine(
                    speaker="æ¤èŒ",
                    text="æˆ‘ä»¬æ­£åœ¨åŠªåŠ›ä¿®å¤ï¼Œè¯·ç¨åå†æ¥æ”¶å¬ã€‚æ„Ÿè°¢ä½ çš„ç†è§£ï¼",
                    emotion="neutral",
                ),
            ],
            duration_estimate=30,
        )

    @staticmethod
    def _format_number(num: int) -> str:
        """æ ¼å¼åŒ–æ•°å­—ï¼ˆä¸‡ã€äº¿ï¼‰"""
        if num >= 100_000_000:
            return f"{num / 100_000_000:.1f}äº¿"
        elif num >= 10_000:
            return f"{num / 10_000:.1f}ä¸‡"
        else:
            return str(num)


# ç¤ºä¾‹ç”¨æ³•
if __name__ == "__main__":
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    from models.topic import XHSTopic, TopicAnalysisResult, AIInsight

    test_topics = [
        XHSTopic(
            topic_id="t1",
            title="æ˜¥èŠ‚å‡ºæ¸¸æ”»ç•¥",
            heat_score=1500000,
            read_count=50000000,
            note_count=20000,
            rank=1,
            rank_change=2,
            trend_icon="â†‘",
            category="æ—…æ¸¸",
        ),
        XHSTopic(
            topic_id="t2",
            title="å¹´è´§æ¸…å•æ¨è",
            heat_score=1200000,
            read_count=40000000,
            note_count=18000,
            rank=2,
            rank_change=-1,
            trend_icon="â†“",
            category="ç¾é£Ÿ",
        ),
    ]

    test_analysis = TopicAnalysisResult(
        date="2026-01-15",
        total_topics=50,
        total_heat=50000000,
        top_keywords=["æ˜¥èŠ‚", "æ—…æ¸¸", "å¹´è´§", "ç¾é£Ÿ", "æ”»ç•¥"],
        category_stats={
            "æ—…æ¸¸": {"count": 15, "total_heat": 20000000},
            "ç¾é£Ÿ": {"count": 12, "total_heat": 15000000},
        },
        top_topics=test_topics,
    )

    test_insight = AIInsight(
        user_behavior=["ç”¨æˆ·å¯¹æ˜¥èŠ‚ç›¸å…³å†…å®¹å…³æ³¨åº¦æ˜¾è‘—ä¸Šå‡"],
        trend_predictions=["é¢„è®¡æœªæ¥ä¸€å‘¨ï¼Œæ˜¥èŠ‚æ—…æ¸¸è¯é¢˜å°†æŒç»­å‡æ¸©"],
        creator_tips=["å»ºè®®åˆ›ä½œè€…æå‰å¸ƒå±€æ˜¥èŠ‚ç›¸å…³å†…å®¹"],
    )

    # ç”Ÿæˆè„šæœ¬
    generator = DialogueWriter()
    script = generator.generate(
        analysis_result=test_analysis,
        ai_insight=test_insight,
        target_duration=600,  # 10åˆ†é’Ÿ
    )

    # ä¿å­˜
    json_path, md_path = script.save_to_file(Path("output/test"))

    print(f"âœ“ è„šæœ¬å·²ç”Ÿæˆ:")
    print(f"  JSON: {json_path}")
    print(f"  Markdown: {md_path}")
    print(f"  å¯¹è¯è¡Œæ•°: {len(script.lines)}")
